# views/chunking_view.py - UI Chunking (Excel theo d√≤ng, Word theo ng·ªØ nghƒ©a) + Vector h√≥a
"""Chunking UI: Excel (by row), Word (semantic). Chunks ƒë∆∞·ª£c vector h√≥a v√† d√πng reverse lookup trong flow ch√≠nh."""
import streamlit as st
from datetime import datetime

from config import init_services
from ai_engine import AIService, suggest_relations
from utils.file_importer import UniversalLoader
from utils.auth_manager import check_permission


def _ensure_chunks_table(supabase):
    """ƒê·∫£m b·∫£o chunks table t·ªìn t·∫°i (schema v6)."""
    try:
        supabase.table("chunks").select("id").limit(1).execute()
        return True
    except Exception:
        return False


def render_chunking_tab(project_id):
    """Tab Chunking - Import Excel (theo d√≤ng) v√† Word (theo ng·ªØ nghƒ©a c√≥ g·∫Øn ng·ªØ c·∫£nh), vector h√≥a."""
    st.subheader("‚úÇÔ∏è Chunking & Vector Store")
    st.caption("Excel: c·∫Øt theo d√≤ng. Word: c·∫Øt theo ƒëo·∫°n ng·ªØ nghƒ©a c√≥ ng·ªØ c·∫£nh. Chunks ƒë∆∞·ª£c vector h√≥a ƒë·ªÉ search trong Chat.")

    if not project_id:
        st.info("üìÅ Ch·ªçn Project tr∆∞·ªõc.")
        return

    services = init_services()
    if not services:
        st.warning("Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c d·ªãch v·ª•.")
        return
    supabase = services["supabase"]

    if not _ensure_chunks_table(supabase):
        st.warning("B·∫£ng chunks ch∆∞a t·ªìn t·∫°i. Ch·∫°y schema_v6_migration.sql trong Supabase.")
        return

    user = st.session_state.get("user")
    user_id = getattr(user, "id", None) if user else None
    user_email = getattr(user, "email", None) if user else None
    can_write = bool(
        project_id and user_id
        and check_permission(str(user_id), user_email or "", project_id, "write")
    )
    if not can_write:
        st.warning("Ch·ªâ user c√≥ quy·ªÅn ghi m·ªõi import chunk.")
        return

    current_arc_id = st.session_state.get("current_arc_id")
    can_delete = check_permission(str(user_id or ""), user_email or "", project_id, "delete")

    tab_excel, tab_word, tab_list = st.tabs(["üìä Excel (theo d√≤ng)", "üìÑ Word (theo ng·ªØ nghƒ©a)", "üìã Chunks ƒë√£ l∆∞u"])

    with tab_excel:
        st.markdown("#### Excel - Chunk theo d√≤ng")
        st.caption("M·ªói d√≤ng Excel = 1 chunk. Metadata: sheet_name, row_index, source_file.")
        try:
            uploaded = st.file_uploader("Ch·ªçn file Excel", type=["xlsx", "xls"], key="chunk_excel_upload")
            if uploaded:
                chunks, err = UniversalLoader.load_excel_as_chunks(uploaded)
                if err:
                    st.error(err)
                elif chunks:
                    st.success(f"ƒê√£ parse {len(chunks)} d√≤ng th√†nh chunks.")
                    preview = st.slider("Xem tr∆∞·ªõc N chunk ƒë·∫ßu", 1, min(20, len(chunks)), 5, key="excel_preview")
                    for i, c in enumerate(chunks[:preview]):
                        meta = c.get("meta_json") or {}
                        sm = meta.get("source_metadata", {})
                        with st.expander(f"Chunk {i+1}: {sm.get('sheet_name','')} row {sm.get('row_index','')}"):
                            st.text(c.get("content", "")[:500])
                    if st.button("üíæ Import & Vector h√≥a (Excel)", type="primary", key="import_excel_chunks"):
                        with st.spinner("ƒêang t·∫°o embedding v√† l∆∞u chunks..."):
                            saved = 0
                            for i, c in enumerate(chunks):
                                content = c.get("content", "") or c.get("raw_content", "")
                                if not content.strip():
                                    continue
                                vec = AIService.get_embedding(content)
                                if vec:
                                    meta = c.get("meta_json") or {}
                                    meta["source_type"] = "excel_row"
                                    payload = {
                                        "story_id": project_id,
                                        "raw_content": content,
                                        "content": content,
                                        "meta_json": meta,
                                        "sort_order": i,
                                        "source_type": "excel_row",
                                    }
                                    try:
                                        payload["embedding"] = vec
                                    except Exception:
                                        pass
                                    if current_arc_id:
                                        payload["arc_id"] = current_arc_id
                                    try:
                                        supabase.table("chunks").insert(payload).execute()
                                        saved += 1
                                    except Exception as e:
                                        if "embedding" in str(e).lower() or "vector" in str(e).lower():
                                            payload.pop("embedding", None)
                                            try:
                                                supabase.table("chunks").insert(payload).execute()
                                                saved += 1
                                            except Exception:
                                                pass
                                        else:
                                            st.error(f"L·ªói chunk {i+1}: {e}")
                            st.success(f"ƒê√£ l∆∞u {saved} chunks.")
                            st.rerun()
        except ImportError as e:
            st.error(f"Thi·∫øu dependency: {e}")

    with tab_word:
        st.markdown("#### Word - Chunk theo ng·ªØ nghƒ©a (c√≥ ng·ªØ c·∫£nh)")
        st.caption("AI t√°ch theo ƒëo·∫°n vƒÉn c√≥ √Ω nghƒ©a, m·ªói chunk g·∫Øn ng·ªØ c·∫£nh (heading/ƒëo·∫°n tr∆∞·ªõc).")
        uploaded_word = st.file_uploader("Ch·ªçn file Word (.docx)", type=["docx"], key="chunk_word_upload")
        if uploaded_word:
            text, err = UniversalLoader.load(uploaded_word)
            if err:
                st.error(err)
            elif text:
                # Chunk theo paragraph c√≥ ng·ªØ c·∫£nh (ƒëo·∫°n tr∆∞·ªõc + ƒëo·∫°n hi·ªán t·∫°i)
                from ai_engine import analyze_split_strategy, execute_split_logic
                strategy = analyze_split_strategy(text, file_type="story", context_hint="ƒêo·∫°n vƒÉn c√≥ √Ω nghƒ©a")
                semantic_chunks = execute_split_logic(text, strategy["split_type"], strategy["split_value"])
                if not semantic_chunks and text:
                    # Fallback: c·∫Øt theo ƒë·ªô d√†i 2000 k√Ω t·ª± c√≥ overlap ng·ªØ c·∫£nh
                    chunk_size = 2000
                    overlap = 200
                    semantic_chunks = []
                    start = 0
                    idx = 1
                    while start < len(text):
                        end = min(start + chunk_size, len(text))
                        part = text[start:end]
                        # Th√™m ng·ªØ c·∫£nh: 100 k√Ω t·ª± tr∆∞·ªõc
                        ctx_start = max(0, start - overlap)
                        context_prefix = text[ctx_start:start] if ctx_start < start else ""
                        full_content = (context_prefix + "\n\n[---]\n\n" + part) if context_prefix else part
                        semantic_chunks.append({
                            "title": f"ƒêo·∫°n {idx}",
                            "content": full_content.strip(),
                            "order": idx
                        })
                        start = end - overlap
                        idx += 1

                if semantic_chunks:
                    st.success(f"ƒê√£ t√°ch {len(semantic_chunks)} ƒëo·∫°n ng·ªØ nghƒ©a.")
                    preview = st.slider("Xem tr∆∞·ªõc N chunk", 1, min(10, len(semantic_chunks)), 3, key="word_preview")
                    for i, c in enumerate(semantic_chunks[:preview]):
                        with st.expander(f"Chunk {i+1}: {c.get('title','')}"):
                            st.text((c.get("content", "") or "")[:600])
                    if st.button("üíæ Import & Vector h√≥a (Word)", type="primary", key="import_word_chunks"):
                        with st.spinner("ƒêang t·∫°o embedding v√† l∆∞u chunks..."):
                            saved = 0
                            for i, c in enumerate(semantic_chunks):
                                content = c.get("content", "") or ""
                                if not content.strip():
                                    continue
                                vec = AIService.get_embedding(content)
                                meta = {
                                    "source_metadata": {
                                        "source_file": getattr(uploaded_word, "name", "uploaded.docx"),
                                        "chunk_index": i + 1,
                                        "source_type": "word_semantic",
                                    },
                                    "source_type": "word_semantic",
                                }
                                payload = {
                                    "story_id": project_id,
                                    "raw_content": content,
                                    "content": content,
                                    "meta_json": meta,
                                    "sort_order": i,
                                    "source_type": "word_semantic",
                                }
                                try:
                                    payload["embedding"] = vec
                                except Exception:
                                    pass
                                if current_arc_id:
                                    payload["arc_id"] = current_arc_id
                                try:
                                    supabase.table("chunks").insert(payload).execute()
                                    saved += 1
                                except Exception as e:
                                    if "embedding" in str(e).lower():
                                        payload.pop("embedding", None)
                                        try:
                                            supabase.table("chunks").insert(payload).execute()
                                            saved += 1
                                        except Exception:
                                            pass
                                    else:
                                        st.error(f"L·ªói chunk {i+1}: {e}")
                            st.success(f"ƒê√£ l∆∞u {saved} chunks Word.")
                            st.rerun()
            else:
                st.info("File r·ªóng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c.")

    with tab_list:
        r = supabase.table("chunks").select("id, content, source_type, meta_json, arc_id").eq("story_id", project_id).order("sort_order").execute()
        chunks_list = r.data or []
        st.metric("T·ªïng chunks", len(chunks_list))
        for c in chunks_list:
            meta = c.get("meta_json") or {}
            sm = meta.get("source_metadata", meta) if isinstance(meta, dict) else {}
            label = sm.get("sheet_name", "") or sm.get("source_file", "") or c.get("source_type", "") or str(c.get("id", ""))[:8]
            with st.expander(f"Chunk: {label} ‚Äî {c.get('content','')[:50]}...", expanded=False):
                st.text(c.get("content", "")[:500])
                if can_delete and st.button("üóëÔ∏è X√≥a", key=f"chunk_del_{c.get('id')}"):
                    supabase.table("chunks").delete().eq("id", c["id"]).execute()
                    st.success("ƒê√£ x√≥a.")
                    st.rerun()
        st.markdown("---")
        with st.expander("üíÄ Danger Zone", expanded=False):
            st.markdown('<div class="danger-zone">', unsafe_allow_html=True)
            if can_delete and chunks_list:
                confirm = st.checkbox("X√≥a s·∫°ch T·∫§T C·∫¢ chunks", key="chunk_confirm_clear")
                if confirm and st.button("üóëÔ∏è X√≥a s·∫°ch Chunks"):
                    supabase.table("chunks").delete().eq("story_id", project_id).execute()
                    st.success("ƒê√£ x√≥a s·∫°ch.")
                    st.rerun()
            st.markdown("</div>", unsafe_allow_html=True)
