import threading
from datetime import datetime

import streamlit as st

from config import Config, init_services, CostManager
from ai_engine import (
    AIService,
    ContextManager,
    SmartAIRouter,
    RuleMiningSystem,
    HybridSearch,
    check_semantic_intent,
)
from persona import PersonaSystem
from utils.auth_manager import check_permission, submit_pending_change
from utils.python_executor import PythonExecutor


def _auto_crystallize_background(project_id, user_id, persona_role):
    """Ch·∫°y ng·∫ßm: crystallize 25 tin (30 - 5) v√† l∆∞u v√†o Bible [CHAT] (ng√†y-stt)."""
    try:
        services = init_services()
        if not services:
            return
        supabase = services["supabase"]
        q = supabase.table("chat_history").select("id, role, content, created_at").eq("story_id", project_id)
        if user_id:
            q = q.eq("user_id", str(user_id))
        r = q.order("created_at", desc=True).limit(35).execute()
        data = list(r.data)[::-1] if r.data else []
        if len(data) < 25:
            return
        to_crystallize = data[:-5]
        chat_text = "\n".join([f"{m['role']}: {m['content']}" for m in to_crystallize])
        summary = RuleMiningSystem.crystallize_session(to_crystallize, persona_role)
        if not summary or summary == "NO_INFO":
            return
        vec = AIService.get_embedding(summary)
        if not vec:
            return
        today = datetime.utcnow().strftime("%Y-%m-%d")
        try:
            log_r = supabase.table("chat_crystallize_log").select("serial_in_day").eq(
                "story_id", project_id
            ).eq("user_id", str(user_id) or "").eq("crystallize_date", today).execute()
            serial = len(log_r.data) + 1 if log_r.data else 1
        except Exception:
            serial = 1
        entity_name = f"[CHAT] {today} chat-{serial}"
        payload = {
            "story_id": project_id,
            "entity_name": entity_name,
            "description": summary,
            "embedding": vec,
            "source_chapter": 0,
        }
        ins = supabase.table("story_bible").insert(payload).execute()
        bible_id = ins.data[0].get("id") if ins.data else None
        try:
            supabase.table("chat_crystallize_log").insert({
                "story_id": project_id,
                "user_id": str(user_id) if user_id else None,
                "crystallize_date": today,
                "serial_in_day": serial,
                "message_count": len(to_crystallize),
                "bible_entry_id": bible_id,
            }).execute()
        except Exception:
            pass
        try:
            from ai_engine import suggest_relations
            suggestions = suggest_relations(summary, project_id)
            for s in (suggestions or []):
                if s.get("kind") == "relation":
                    try:
                        supabase.table("entity_relations").insert({
                            "source_entity_id": s["source_entity_id"],
                            "target_entity_id": s["target_entity_id"],
                            "relation_type": s.get("relation_type", "li√™n quan"),
                            "description": s.get("description", ""),
                            "story_id": project_id,
                        }).execute()
                    except Exception:
                        pass
        except Exception:
            pass
    except Exception as e:
        print(f"auto_crystallize_background error: {e}")


def render_chat_tab(project_id, persona):
    """Tab Chat - AI Conversation v·ªõi t√≠nh nƒÉng n√¢ng cao. Persona c√≥ th·ªÉ ch·ªçn l·∫°i trong tab."""
    st.header("üí¨ Smart AI Chat")

    col_chat, col_memory = st.columns([3, 1])

    # Th√¥ng tin user & quy·ªÅn: d√πng cho Rule Mining, Crystallize, quy·ªÅn ghi/ch·ªù duy·ªát
    user = st.session_state.get("user")
    user_id = getattr(user, "id", None) if user else None
    user_email = getattr(user, "email", None) if user else None
    can_write = bool(
        project_id
        and user_id
        and check_permission(str(user_id), user_email or "", project_id, "write")
    )
    can_request = bool(
        project_id
        and user_id
        and check_permission(str(user_id), user_email or "", project_id, "request_write")
    )

    with col_memory:
        st.write("### üß† Memory & Settings")
        available = PersonaSystem.get_available_personas()
        default_key = st.session_state.get("persona", "Writer")
        idx = available.index(default_key) if default_key in available else 0
        selected_persona_key = st.selectbox(
            "Persona tr·∫£ l·ªùi",
            available,
            index=idx,
            key="chat_persona_key",
            help="Ch·ªçn persona ƒë·ªÉ AI tr·∫£ l·ªùi theo phong c√°ch n√†y."
        )
        active_persona = PersonaSystem.get_persona(selected_persona_key)

        if st.button("üßπ Clear Screen", use_container_width=True):
            st.session_state['chat_cutoff'] = datetime.utcnow().isoformat()
            st.rerun()

        if st.button("üîÑ Show All", use_container_width=True):
            st.session_state['chat_cutoff'] = "1970-01-01"
            st.rerun()

        st.session_state['enable_history'] = st.toggle(
            "üíæ Save Chat History",
            value=True,
            help="Turn off for anonymous chat (Not saved to DB, AI doesn't learn)"
        )

        st.session_state['strict_mode'] = st.toggle(
            "üö´ Strict Mode",
            value=False,
            help="ON: AI only answers based on found data. No fabrication. (Temp = 0)"
        )
        st.session_state['router_ignore_history'] = st.toggle(
            "‚ö°Ô∏è Router Ignore History",
            value=False,
            help="B·∫≠t c√°i n√†y ƒë·ªÉ Router ch·ªâ ph√¢n t√≠ch c√¢u hi·ªán t·∫°i, kh√¥ng b·ªã nhi·ªÖu b·ªüi chat c≈©."
        )
        st.divider()
        st.write("### üï∞Ô∏è Context Depth")
        st.session_state["history_depth"] = st.slider(
            "Chat History Limit",
            min_value=0,
            max_value=30,
            value=st.session_state.get("history_depth", 5),
            step=1,
            help="S·ªë l∆∞·ª£ng tin nh·∫Øn c≈© g·ª≠i k√®m. C√†ng cao c√†ng nh·ªõ dai nh∆∞ng t·ªën ti·ªÅn h∆°n.",
            key="chat_history_depth",
        )

        st.caption("üíé Auto Crystallize: M·ªói 30 tin nh·∫Øn, h·ªá th·ªëng t·ª± t√≥m t·∫Øt & l∆∞u Bible [CHAT] (ng√†y-stt).")

    @st.fragment
    def _chat_messages_fragment():
        try:
            services = init_services()
            supabase = services["supabase"]
            # Chat ri√™ng t∆∞: ch·ªâ l·∫•y l·ªãch s·ª≠ chat c·ªßa ch√≠nh user hi·ªán t·∫°i
            q = (
                supabase.table("chat_history")
                .select("*")
                .eq("story_id", project_id)
            )
            if user_id:
                q = q.eq("user_id", str(user_id))
            msgs_data = (
                q.order("created_at", desc=True)
                .limit(50)
                .execute()
            )
            msgs = msgs_data.data[::-1] if msgs_data.data else []
            visible_msgs = [m for m in msgs if m["created_at"] > st.session_state.get("chat_cutoff", "1970-01-01")]
            for m in visible_msgs:
                role_icon = active_persona["icon"] if m["role"] == "model" else None
                with st.chat_message(m["role"], avatar=role_icon):
                    st.markdown(m["content"])
                    if m.get("metadata"):
                        with st.expander("üìä Details"):
                            st.json(m["metadata"], expanded=False)
        except Exception as e:
            st.error(f"Error loading history: {e}")
        history_depth = st.session_state.get("history_depth", 5)
        if prompt := st.chat_input(f"Ask {active_persona['icon']} AI Assistant...", key="chat_input_main"):
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.spinner("Thinking..."):
                now_timestamp = datetime.utcnow().isoformat()

                if st.session_state.get('router_ignore_history'):
                    recent_history_text = "NO_HISTORY_AVAILABLE (User requested to ignore context)"
                else:
                    recent_history_text = "\n".join([
                        f"{m['role']}: {m['content']}"
                        for m in visible_msgs[-5:]
                    ])

                # Semantic Intent: n·∫øu kh·ªõp >= ng∆∞·ª°ng th√¨ d√πng data tr·ª±c ti·∫øp (kh√¥ng c·∫ßn intent)
                semantic_match = None
                try:
                    svc = init_services()
                    if svc:
                        r = svc["supabase"].table("settings").select("value").eq("key", "semantic_intent_no_use").execute()
                        no_use = r.data and r.data[0] and int(r.data[0].get("value", 0)) == 1
                        if not no_use:
                            semantic_match = check_semantic_intent(prompt, project_id)
                except Exception:
                    semantic_match = check_semantic_intent(prompt, project_id)
                if semantic_match:
                    router_out = {"intent": "chat_casual", "target_files": [], "target_bible_entities": [], "rewritten_query": prompt, "chapter_range": None, "chapter_range_mode": None, "chapter_range_count": 5}
                    if semantic_match.get("related_data"):
                        router_out["_semantic_data"] = semantic_match["related_data"]
                    debug_notes.append(f"üéØ Semantic match {int(semantic_match.get('similarity',0)*100)}%")
                else:
                    router_out = SmartAIRouter.ai_router_pro_v2(prompt, recent_history_text, project_id)
                intent = router_out.get('intent', 'chat_casual')
                targets = router_out.get('target_files', [])
                rewritten_query = router_out.get('rewritten_query', prompt)

                debug_notes = [f"Intent: {intent}"]
                if st.session_state.get('router_ignore_history'):
                    debug_notes.append("‚ö°Ô∏è Router: Ignored History")

                exec_result = None
                if intent == "numerical_calculation":
                    context_text, sources, context_tokens = ContextManager.build_context(
                        router_out, project_id, active_persona,
                        st.session_state.get('strict_mode', False),
                        current_arc_id=st.session_state.get('current_arc_id'),
                        session_state=dict(st.session_state),
                    )
                    code_prompt = f"""User h·ªèi: "{prompt}"
Context c√≥ s·∫µn:
{context_text[:6000]}

Nhi·ªám v·ª•: T·∫°o code Python (pandas/numpy) ƒë·ªÉ tr·∫£ l·ªùi. G√°n k·∫øt qu·∫£ cu·ªëi v√†o bi·∫øn result.
Ch·ªâ tr·∫£ v·ªÅ code trong block ```python ... ```, kh√¥ng gi·∫£i th√≠ch."""
                    try:
                        code_resp = AIService.call_openrouter(
                            messages=[{"role": "user", "content": code_prompt}],
                            model=st.session_state.get('selected_model', Config.DEFAULT_MODEL),
                            temperature=0.1,
                            max_tokens=2000,
                        )
                        raw = (code_resp.choices[0].message.content or "").strip()
                        import re
                        m = re.search(r'```(?:python)?\s*(.*?)```', raw, re.DOTALL)
                        code = m.group(1).strip() if m else raw
                        if code:
                            val, err = PythonExecutor.execute(code, result_variable="result")
                            if err:
                                exec_result = f"(Executor l·ªói: {err})"
                            else:
                                exec_result = str(val) if val is not None else "null"
                                debug_notes.append("üßÆ Python Executor OK")
                    except Exception as ex:
                        exec_result = f"(L·ªói: {ex})"
                    if exec_result:
                        context_text += f"\n\n--- K·∫æT QU·∫¢ T√çNH TO√ÅN (Python Executor) ---\n{exec_result}"

                if exec_result is None:
                    context_text, sources, context_tokens = ContextManager.build_context(
                        router_out,
                        project_id,
                        active_persona,
                        st.session_state.get('strict_mode', False),
                        current_arc_id=st.session_state.get('current_arc_id'),
                        session_state=dict(st.session_state),
                    )
                    if router_out.get("_semantic_data"):
                        context_text = f"[SEMANTIC INTENT - Data]\n{router_out['_semantic_data']}\n\n{context_text}"
                        sources.append("üéØ Semantic Intent")

                debug_notes.extend(sources)

                final_prompt = f"CONTEXT:\n{context_text}\n\nUSER QUERY: {prompt}"

                run_instruction = active_persona['core_instruction']
                run_temperature = st.session_state.get('temperature', 0.7)

                if st.session_state.get('strict_mode'):
                    run_temperature = 0.0

                messages = []
                system_message = f"""{run_instruction}

            TH√îNG TIN NG·ªÆ C·∫¢NH (CONTEXT):
            {context_text}

            H∆Ø·ªöNG D·∫™N:
            - Tr·∫£ l·ªùi d·ª±a tr√™n Context n·∫øu c√≥.
            - H·ªØu √≠ch, s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
            - Ch·∫ø ƒë·ªô hi·ªán t·∫°i: {active_persona['role']}
            - Ng√¥n ng·ªØ: ∆Øu ti√™n Ti·∫øng Vi·ªát (tr·ª´ khi User y√™u c·∫ßu kh√°c ho·∫∑c code).
            """

                messages.append({"role": "system", "content": system_message})

                depth = history_depth
                if depth > 0:
                    past_chats = visible_msgs[-depth:]
                else:
                    past_chats = []

                for msg in past_chats:
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })

                if len(past_chats) > 5:
                    debug_notes.append(f"üìö Memory: Last {len(past_chats)} msgs")

                messages.append({"role": "user", "content": prompt})

                try:
                    model = st.session_state.get('selected_model', Config.DEFAULT_MODEL)

                    response = AIService.call_openrouter(
                        messages=messages,
                        model=model,
                        temperature=run_temperature,
                        max_tokens=active_persona.get('max_tokens', 4000),
                        stream=True
                    )

                    with st.chat_message("assistant", avatar=active_persona['icon']):
                        if debug_notes:
                            st.caption(f"üß† {', '.join(debug_notes)}")
                        if st.session_state.get('strict_mode'):
                            st.caption("üîí Strict Mode: ON")

                        full_response_text = ""
                        placeholder = st.empty()

                        for chunk in response:
                            if chunk.choices[0].delta.content is not None:
                                content = chunk.choices[0].delta.content
                                full_response_text += content
                                placeholder.markdown(full_response_text + "‚ñå")

                        placeholder.markdown(full_response_text)

                    input_tokens = AIService.estimate_tokens(system_message + prompt)
                    output_tokens = AIService.estimate_tokens(full_response_text)
                    cost = AIService.calculate_cost(input_tokens, output_tokens, model)

                    if 'user' in st.session_state:
                        CostManager.update_budget(st.session_state.user.id, cost)

                    if full_response_text and st.session_state.get('enable_history', True):
                        services = init_services()
                        supabase = services['supabase']

                        supabase.table("chat_history").insert([
                            {
                                "story_id": project_id,
                                "user_id": str(user_id) if user_id else None,
                                "role": "user",
                                "content": prompt,
                                "created_at": now_timestamp,
                                "metadata": {
                                    "intent": intent,
                                    "router_output": router_out,
                                    "model": model,
                                    "temperature": run_temperature
                                }
                            },
                            {
                                "story_id": project_id,
                                "user_id": str(user_id) if user_id else None,
                                "role": "model",
                                "content": full_response_text,
                                "created_at": now_timestamp,
                                "metadata": {
                                    "model": model,
                                    "cost": f"${cost:.6f}",
                                    "tokens": input_tokens + output_tokens
                                }
                            }
                        ]).execute()

                        # Auto crystallize m·ªói 30 tin (ch·∫°y ng·∫ßm)
                        if can_write and user_id:
                            try:
                                count_r = supabase.table("chat_history").select("id", count="exact").eq(
                                    "story_id", project_id
                                ).eq("user_id", str(user_id)).execute()
                                total = getattr(count_r, "count", 0) or len(count_r.data or [])
                                if total >= 30 and total % 30 == 0:
                                    threading.Thread(
                                        target=_auto_crystallize_background,
                                        args=(project_id, user_id, active_persona["role"]),
                                        daemon=True,
                                    ).start()
                            except Exception:
                                pass

                        # Rule mining
                        if can_write:
                            new_rule = RuleMiningSystem.extract_rule_raw(prompt, full_response_text)
                            if new_rule:
                                st.session_state['pending_new_rule'] = new_rule
                            # Offer add to Semantic Intent (n·∫øu b·∫≠t auto-create v√† kh√¥ng ph·∫£i chat phi·∫øm)
                            try:
                                r = init_services()["supabase"].table("settings").select("value").eq("key", "semantic_intent_no_auto_create").execute()
                                no_auto = r.data and r.data[0] and int(r.data[0].get("value", 0)) == 1
                            except Exception:
                                no_auto = False
                            if not no_auto and intent != "chat_casual":
                                st.session_state["pending_semantic_add"] = {"prompt": prompt, "response": full_response_text, "context": context_text, "intent": intent}

                    elif not st.session_state.get('enable_history', True):
                        st.caption("üëª Anonymous mode: History not saved & Rule mining disabled.")

                except Exception as e:
                    st.error(f"Generation error: {str(e)}")

    with col_chat:
        _chat_messages_fragment()

    # Offer add to Semantic Intent
    if "pending_semantic_add" in st.session_state and can_write:
        p = st.session_state["pending_semantic_add"]
        with st.expander("üéØ Th√™m v√†o Semantic Intent?", expanded=True):
            st.caption("C√¢u h·ªèi v·ª´a r·ªìi kh√¥ng ph·∫£i chat phi·∫øm. Th√™m l√†m m·∫´u ƒë·ªÉ l·∫ßn sau kh·ªõp nhanh?")
            st.write("**C√¢u h·ªèi:**", p.get("prompt", "")[:100])
            col_a, col_b = st.columns(2)
            with col_a:
                if st.button("‚úÖ Th√™m v√†o Semantic"):
                    def _add_semantic():
                        try:
                            svc = init_services()
                            if not svc:
                                return
                            sb = svc["supabase"]
                            vec = AIService.get_embedding(p.get("prompt", ""))
                            ctx = p.get("context", "") or ""
                            resp = p.get("response", "") or ""
                            related_data = (ctx.rstrip() + "\n\n--- C√¢u tr·∫£ l·ªùi ---\n" + resp) if ctx else resp
                            payload = {"story_id": project_id, "question_sample": p.get("prompt", ""), "intent": "chat_casual", "related_data": related_data}
                            if vec:
                                payload["embedding"] = vec
                            try:
                                sb.table("semantic_intent").insert(payload).execute()
                            except Exception:
                                payload.pop("embedding", None)
                                sb.table("semantic_intent").insert(payload).execute()
                        except Exception:
                            pass
                    threading.Thread(target=_add_semantic, daemon=True).start()
                    del st.session_state["pending_semantic_add"]
                    st.toast("ƒê√£ th√™m v√†o Semantic Intent (ch·∫°y ng·∫ßm).")
                    st.rerun()
            with col_b:
                if st.button("‚ùå B·ªè qua"):
                    del st.session_state["pending_semantic_add"]
                    st.rerun()

    # Rule Mining UI
    if 'pending_new_rule' in st.session_state and can_write:
        rule_content = st.session_state['pending_new_rule']

        with st.expander("üßê AI discovered a new Rule!", expanded=True):
            st.write(f"**Content:** {rule_content}")

            if st.session_state.get('rule_analysis') is None:
                with st.spinner("Checking for conflicts..."):
                    st.session_state['rule_analysis'] = RuleMiningSystem.analyze_rule_conflict(rule_content, project_id)

            analysis = st.session_state['rule_analysis']
            if analysis:
                st.info(f"AI Assessment: **{analysis.get('status', 'UNKNOWN')}** - {analysis.get('reason', 'N/A')}")
                if analysis['status'] == "CONFLICT":
                    st.warning(f"‚ö†Ô∏è Conflict with: {analysis['existing_rule_summary']}")
                elif analysis['status'] == "MERGE":
                    st.info(f"üí° Merge suggestion: {analysis['merged_content']}")
            else:
                st.error("Could not analyze rule conflict.")

            c1, c2, c3 = st.columns(3)

            if c1.button("‚úÖ Save/Merge Rule"):
                final_content = analysis.get('merged_content') if analysis and analysis['status'] == "MERGE" else rule_content
                vec = AIService.get_embedding(final_content)

                services = init_services()
                supabase = services['supabase']

                payload = {
                    "entity_name": f"[RULE] {datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    "description": final_content,
                    "embedding": vec,
                    "source_chapter": 0,
                }
                try:
                    if can_write:
                        payload["story_id"] = project_id
                        supabase.table("story_bible").insert(payload).execute()
                        st.toast("Learned new rule!")
                    elif can_request:
                        pid = submit_pending_change(
                            story_id=project_id,
                            requested_by_email=user_email or "",
                            table_name="story_bible",
                            target_key={},
                            old_data={},
                            new_data=payload,
                        )
                        if pid:
                            st.toast("ƒê√£ g·ª≠i y√™u c·∫ßu th√™m RULE cho Owner duy·ªát.", icon="üì§")
                        else:
                            st.error("Kh√¥ng g·ª≠i ƒë∆∞·ª£c y√™u c·∫ßu (ki·ªÉm tra b·∫£ng pending_changes).")
                    else:
                        st.warning("B·∫°n kh√¥ng c√≥ quy·ªÅn l∆∞u ho·∫∑c g·ª≠i y√™u c·∫ßu Rule.")
                except Exception as e:
                    st.error(f"L·ªói khi l∆∞u RULE: {e}")

                del st.session_state['pending_new_rule']
                del st.session_state['rule_analysis']
                st.rerun()

            if c2.button("‚úèÔ∏è Edit then Save"):
                st.session_state['edit_rule_manual'] = rule_content

            if c3.button("‚ùå Ignore"):
                del st.session_state['pending_new_rule']
                del st.session_state['rule_analysis']
                st.rerun()

        if 'edit_rule_manual' in st.session_state and can_write:
            edited = st.text_input("Edit rule:", value=st.session_state['edit_rule_manual'])
            if st.button("Save edited version"):
                vec = AIService.get_embedding(edited)

                services = init_services()
                supabase = services['supabase']

                supabase.table("story_bible").insert({
                    "story_id": project_id,
                    "entity_name": "[RULE] Manual",
                    "description": edited,
                    "embedding": vec,
                    "source_chapter": 0
                }).execute()

                del st.session_state['pending_new_rule']
                del st.session_state['rule_analysis']
                del st.session_state['edit_rule_manual']
                st.rerun()
