# ai_engine.py - AI Service, Router, Context, Rule Mining
import json
import re
from collections import defaultdict
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Tuple, Any

import streamlit as st
from openai import OpenAI

from config import Config, init_services

try:
    from core.arc_service import ArcService
    from core.reverse_lookup import ReverseLookupAssembler
except ImportError:
    ArcService = None
    ReverseLookupAssembler = None


# ==========================================
# ü§ñ AI SERVICE
# ==========================================
class AIService:
    """D·ªãch v·ª• AI s·ª≠ d·ª•ng OpenAI client cho OpenRouter v·ªõi c√°c t√≠nh nƒÉng n√¢ng cao"""

    @staticmethod
    @st.cache_data(ttl=3600)
    def get_available_models():
        """L·∫•y danh s√°ch model c√≥ s·∫µn t·ª´ OpenRouter"""
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )
            return Config.AVAILABLE_MODELS
        except Exception:
            return Config.AVAILABLE_MODELS

    @staticmethod
    def call_openrouter(
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 8000,
        stream: bool = False,
        response_format: Optional[Dict] = None
    ) -> Any:
        """G·ªçi OpenRouter API s·ª≠ d·ª•ng OpenAI client"""
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY,
                default_headers={
                    "HTTP-Referer": "https://v-universe.streamlit.app",
                    "X-Title": "V-Universe AI Hub"
                }
            )

            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream,
                response_format=response_format
            )

            return response
        except Exception as e:
            raise Exception(f"OpenRouter API error: {str(e)}")

    @staticmethod
    def get_embedding(text: str) -> Optional[List[float]]:
        """L·∫•y embedding t·ª´ OpenRouter"""
        if not text or not isinstance(text, str) or not text.strip():
            return None

        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )

            response = client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=text
            )

            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @staticmethod
    def get_embeddings_batch(texts: List[str], batch_size: int = 100) -> List[Optional[List[float]]]:
        """L·∫•y embedding h√†ng lo·∫°t (nhi·ªÅu text trong √≠t request). Tr·∫£ v·ªÅ list c√πng th·ª© t·ª± v·ªõi texts; ph·∫ßn t·ª≠ l·ªói l√† None."""
        if not texts:
            return []
        out: List[Optional[List[float]]] = [None] * len(texts)
        valid_indices: List[int] = []
        valid_texts: List[str] = []
        for i, t in enumerate(texts):
            if t and isinstance(t, str) and t.strip():
                valid_indices.append(i)
                valid_texts.append(t.strip())
        if not valid_texts:
            return out
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )
            for start in range(0, len(valid_texts), batch_size):
                chunk = valid_texts[start:start + batch_size]
                chunk_indices = valid_indices[start:start + batch_size]
                response = client.embeddings.create(
                    model=Config.EMBEDDING_MODEL,
                    input=chunk
                )
                for j, emb_obj in enumerate(response.data):
                    idx = chunk_indices[j] if j < len(chunk_indices) else start + j
                    if idx < len(out) and emb_obj.embedding is not None:
                        out[idx] = emb_obj.embedding
        except Exception as e:
            print(f"Embedding batch error: {e}")
        return out

    @staticmethod
    def estimate_tokens(text: str) -> int:
        """∆Ø·ªõc t√≠nh s·ªë token"""
        if not text:
            return 0
        return len(text) // 4

    @staticmethod
    def calculate_cost(
        input_tokens: int,
        output_tokens: int,
        model: str
    ) -> float:
        """T√≠nh chi ph√≠ cho request"""
        model_costs = Config.MODEL_COSTS.get(model, {"input": 0.0, "output": 0.0})

        input_cost = (input_tokens / 1_000_000) * model_costs["input"]
        output_cost = (output_tokens / 1_000_000) * model_costs["output"]

        return round(input_cost + output_cost, 6)

    @staticmethod
    def clean_json_text(text):
        """L√†m s·∫°ch markdown (```json ... ```) tr∆∞·ªõc khi parse"""
        if not text:
            return "{}"
        text = text.replace("```json", "").replace("```", "").strip()
        start = text.find("{")
        end = text.rfind("}") + 1
        if start != -1 and end != 0:
            return text[start:end]
        return text


def cap_context_to_tokens(text: str, max_tokens: int) -> Tuple[str, int]:
    """Ki·ªÉm tra v√† c·∫Øt context sao cho kh√¥ng v∆∞·ª£t qu√° max_tokens. C·∫Øt t·ª´ cu·ªëi ƒë·ªÉ gi·ªØ ph·∫ßn ƒë·∫ßu (persona, rules...)."""
    if not text or max_tokens <= 0:
        return text or "", AIService.estimate_tokens(text or "")
    est = AIService.estimate_tokens(text)
    if est <= max_tokens:
        return text, est
    # ∆Ø·ªõc t√≠nh: estimate_tokens = len//4, n√™n target_chars ‚âà max_tokens * 4
    target_chars = max_tokens * 4
    out = text[:target_chars] if len(text) > target_chars else text
    est = AIService.estimate_tokens(out)
    while est > max_tokens and len(out) > 500:
        out = out[:-500]
        est = AIService.estimate_tokens(out)
    return out, est


# ==========================================
# üîç HYBRID SEARCH SYSTEM (V5 - Re-ranking + lookup stats)
# ==========================================
# Tr·ªçng s·ªë re-rank: VectorSim * 0.7 + RecencyBonus * 0.1 + ImportanceBias * 0.2
VECTOR_WEIGHT = 0.7
RECENCY_WEIGHT = 0.1
IMPORTANCE_WEIGHT = 0.2
RECENCY_BONUS_HOURS = 24


def _safe_float(value: Any, default: float = 0.5) -> float:
    """L·∫•y s·ªë th·ª±c an to√†n t·ª´ record (defensive)."""
    if value is None:
        return default
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _recency_bonus(last_lookup_at: Any) -> float:
    """RecencyBonus: 1.0 n·∫øu last_lookup_at trong v√≤ng 24h, else 0.0."""
    if last_lookup_at is None:
        return 0.0
    try:
        if isinstance(last_lookup_at, str):
            dt = datetime.fromisoformat(last_lookup_at.replace("Z", "+00:00"))
        else:
            dt = last_lookup_at
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        delta = now - dt
        return 1.0 if delta <= timedelta(hours=RECENCY_BONUS_HOURS) else 0.0
    except Exception:
        return 0.0


def _rerank_by_score(rows: List[Dict], top_k: int) -> List[Dict]:
    """T√≠nh Final Score v√† s·∫Øp x·∫øp l·∫°i: (VectorSim*0.7) + (RecencyBonus*0.1) + (ImportanceBias*0.2)."""
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        item["_final_score"] = (vector_sim * VECTOR_WEIGHT) + (recency * RECENCY_WEIGHT) + (importance * IMPORTANCE_WEIGHT)
    sorted_rows = sorted(rows, key=lambda x: x.get("_final_score", 0.0), reverse=True)
    for item in sorted_rows:
        item.pop("_final_score", None)
    return sorted_rows[:top_k]


def _rerank_by_score_with_breakdown(rows: List[Dict], top_k: int) -> List[Dict]:
    """Gi·ªëng _rerank_by_score nh∆∞ng gi·ªØ l·∫°i score_vector, score_recency, score_bias, score_final ƒë·ªÉ hi·ªÉn th·ªã."""
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        item["score_vector"] = round(vector_sim * VECTOR_WEIGHT, 4)
        item["score_recency"] = round(recency * RECENCY_WEIGHT, 4)
        item["score_bias"] = round(importance * IMPORTANCE_WEIGHT, 4)
        item["score_final"] = round(
            item["score_vector"] + item["score_recency"] + item["score_bias"], 4
        )
    sorted_rows = sorted(rows, key=lambda x: x.get("score_final", 0.0), reverse=True)
    return sorted_rows[:top_k]


class HybridSearch:
    """H·ªá th·ªëng t√¨m ki·∫øm k·∫øt h·ª£p vector v√† t·ª´ kh√≥a (V5: re-ranking, lookup_count, last_lookup_at)"""

    @staticmethod
    def smart_search_hybrid_raw(
        query_text: str,
        project_id: str,
        top_k: int = 10,
        inferred_prefixes: Optional[List[str]] = None,
    ) -> List[Dict]:
        """T√¨m ki·∫øm hybrid tr·∫£ v·ªÅ raw data; re-rank trong Python. N·∫øu inferred_prefixes c√≥ gi√° tr·ªã th√¨ d√πng prefix-aware rerank."""
        try:
            services = init_services()
            supabase = services["supabase"]
            query_vec = AIService.get_embedding(query_text)
            candidate_limit = max(top_k * 3, 30)

            if query_vec:
                try:
                    response = supabase.rpc("hybrid_search", {
                        "query_text": query_text,
                        "query_embedding": query_vec,
                        "match_threshold": 0.3,
                        "match_count": candidate_limit,
                        "story_id_input": project_id,
                    }).execute()
                    raw_list = response.data if response.data else []
                except Exception:
                    raw_list = []
                if not raw_list:
                    try:
                        response = supabase.table("story_bible").select("*").eq(
                            "story_id", project_id
                        ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                            candidate_limit
                        ).execute()
                        raw_list = response.data if response.data else []
                        for item in raw_list:
                            item["similarity"] = 0.5
                    except Exception:
                        raw_list = []
            else:
                try:
                    response = supabase.table("story_bible").select("*").eq(
                        "story_id", project_id
                    ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                        candidate_limit
                    ).execute()
                    raw_list = response.data if response.data else []
                    for item in raw_list:
                        item["similarity"] = 0.5
                except Exception:
                    raw_list = []

            if not raw_list:
                return []

            if inferred_prefixes:
                reranked = _rerank_by_score_with_prefix(raw_list, top_k, inferred_prefixes)
            else:
                reranked = _rerank_by_score(raw_list, top_k)
            return reranked

        except Exception as e:
            print(f"Search error: {e}")
            return []

    @staticmethod
    def smart_search_hybrid_raw_with_scores(query_text: str, project_id: str, top_k: int = 10) -> List[Dict]:
        """Gi·ªëng smart_search_hybrid_raw nh∆∞ng m·ªói item c√≥ th√™m score_vector, score_recency, score_bias, score_final."""
        try:
            services = init_services()
            supabase = services["supabase"]
            query_vec = AIService.get_embedding(query_text)
            candidate_limit = max(top_k * 3, 30)
            if query_vec:
                try:
                    response = supabase.rpc("hybrid_search", {
                        "query_text": query_text,
                        "query_embedding": query_vec,
                        "match_threshold": 0.3,
                        "match_count": candidate_limit,
                        "story_id_input": project_id,
                    }).execute()
                    raw_list = response.data if response.data else []
                except Exception:
                    raw_list = []
                if not raw_list:
                    try:
                        response = supabase.table("story_bible").select("*").eq(
                            "story_id", project_id
                        ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                            candidate_limit
                        ).execute()
                        raw_list = response.data if response.data else []
                        for item in raw_list:
                            item["similarity"] = 0.5
                    except Exception:
                        raw_list = []
            else:
                try:
                    response = supabase.table("story_bible").select("*").eq(
                        "story_id", project_id
                    ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                        candidate_limit
                    ).execute()
                    raw_list = response.data if response.data else []
                    for item in raw_list:
                        item["similarity"] = 0.5
                except Exception:
                    raw_list = []
            if not raw_list:
                return []
            return _rerank_by_score_with_breakdown(raw_list, top_k)
        except Exception as e:
            print(f"Search error: {e}")
            return []

    @staticmethod
    def update_lookup_stats(entity_id: Any) -> None:
        """TƒÉng lookup_count += 1 v√† c·∫≠p nh·∫≠t last_lookup_at = now() cho record v·ª´a ƒë∆∞·ª£c t√¨m th·∫•y. Defensive: kh√¥ng crash n·∫øu c·ªôt ch∆∞a c√≥."""
        if entity_id is None:
            return
        try:
            services = init_services()
            if not services:
                return
            supabase = services["supabase"]
            now_iso = datetime.now(timezone.utc).isoformat()
            try:
                row = supabase.table("story_bible").select("lookup_count").eq("id", entity_id).execute()
                current = 0
                if row.data and len(row.data) > 0:
                    current = _safe_float(row.data[0].get("lookup_count"), 0.0)
                new_count = int(current) + 1
                supabase.table("story_bible").update({
                    "lookup_count": new_count,
                    "last_lookup_at": now_iso,
                }).eq("id", entity_id).execute()
            except Exception:
                pass
        except Exception as e:
            print(f"update_lookup_stats error: {e}")

    @staticmethod
    def smart_search_hybrid(query_text: str, project_id: str, top_k: int = 10) -> str:
        """Wrapper tr·∫£ v·ªÅ string context (gi·ªØ t∆∞∆°ng th√≠ch)."""
        raw_data = HybridSearch.smart_search_hybrid_raw(query_text, project_id, top_k)
        results = []
        if raw_data:
            for item in raw_data:
                name = item.get("entity_name") or ""
                desc = item.get("description") or ""
                results.append(f"- [{name}]: {desc}")
        return "\n".join(results) if results else ""


# ==========================================
# üéØ SEMANTIC INTENT (tr∆∞·ªõc Router - kh·ªõp th√¨ b·ªè qua Router)
# ==========================================
def check_semantic_intent(
    query_text: str,
    project_id: str,
    threshold: float = 0.90,
) -> Optional[Dict]:
    """So s√°nh vector c√¢u h·ªèi v·ªõi semantic_intent. N·∫øu kh·ªõp >= threshold th√¨ tr·∫£ v·ªÅ row (related_data ch√≠nh), else None. Kh√¥ng c·∫ßn intent."""
    if not query_text or not project_id:
        return None
    try:
        services = init_services()
        if not services:
            return None
        supabase = services["supabase"]
        try:
            supabase.table("semantic_intent").select("id").limit(1).execute()
        except Exception:
            return None
        try:
            r = supabase.table("settings").select("value").eq("key", "semantic_intent_threshold").execute()
            if r.data and r.data[0]:
                t = r.data[0].get("value")
                threshold = max(0.85, min(1.0, float(t) / 100.0)) if t is not None else threshold
        except Exception:
            pass
        query_vec = AIService.get_embedding(query_text)
        if not query_vec:
            return None
        rows = supabase.table("semantic_intent").select("id, question_sample, intent, related_data, embedding").eq("story_id", project_id).execute()
        data = rows.data or []
        best_match = None
        best_sim = 0.0
        for row in data:
            emb = row.get("embedding")
            if emb is None:
                continue
            if isinstance(emb, str):
                try:
                    emb = json.loads(emb)
                except Exception:
                    continue
            try:
                import math
                dot = sum(a * b for a, b in zip(query_vec, emb))
                na = math.sqrt(sum(a * a for a in query_vec))
                nb = math.sqrt(sum(b * b for b in emb))
                sim = dot / (na * nb) if na and nb else 0
                sim = (sim + 1) / 2
                if sim >= threshold and sim > best_sim:
                    best_sim = sim
                    best_match = {**row, "similarity": sim}
            except Exception:
                pass
        return best_match
    except Exception as e:
        print(f"check_semantic_intent error: {e}")
        return None


# ==========================================
# üì¶ CHUNK SEARCH (vector + text, reverse lookup)
# ==========================================
def search_chunks_vector(
    query_text: str,
    project_id: str,
    arc_id: Optional[str] = None,
    top_k: int = 10,
) -> List[Dict]:
    """T√¨m chunks theo vector (n·∫øu c√≥ embedding) ho·∫∑c text fallback. Tr·∫£ v·ªÅ list chunk rows. N·∫øu c√≥ arc_id m√† kh√¥ng c√≥ k·∫øt qu·∫£ th√¨ th·ª≠ l·∫°i kh√¥ng l·ªçc arc."""
    try:
        services = init_services()
        if not services:
            return []
        supabase = services["supabase"]
        query_vec = AIService.get_embedding(query_text)
        q = supabase.table("chunks").select("id, chapter_id, arc_id, content, raw_content, meta_json, story_id").eq("story_id", project_id)
        if arc_id:
            q = q.eq("arc_id", arc_id)
        if query_vec:
            try:
                r = supabase.rpc("hybrid_chunk_search", {
                    "query_text": query_text,
                    "query_embedding": query_vec,
                    "story_id_input": project_id,
                    "match_threshold": 0.3,
                    "match_count": top_k,
                }).execute()
                rows = list(r.data) if r.data else []
                if arc_id and not rows and query_text and query_text.strip():
                    rows = search_chunks_vector(query_text, project_id, arc_id=None, top_k=top_k)
                return rows
            except Exception:
                pass
        if query_text and query_text.strip():
            pattern = "%" + str(query_text).strip() + "%"
            r = q.ilike("content", pattern).limit(top_k).execute()
            rows = list(r.data) if r.data else []
            if arc_id and not rows:
                rows = search_chunks_vector(query_text, project_id, arc_id=None, top_k=top_k)
            return rows
        return []
    except Exception as e:
        print(f"search_chunks_vector error: {e}")
        return []


# ==========================================
# üß≠ SMART AI ROUTER SYSTEM
# ==========================================


def extract_prefix(name: str) -> Tuple[str, str]:
    """
    B√≥c t√°ch ti·ªÅn t·ªë: t√¨m n·ªôi dung trong [...] ·ªü ƒë·∫ßu chu·ªói.
    VD: "[V≈® KH√ç] Ki·∫øm Thi√™n" -> ("V≈® KH√ç", "Ki·∫øm Thi√™n"). Defensive: l·ªói -> ("", name g·ªëc).
    """
    if not name or not isinstance(name, str):
        return "", (name or "")
    s = name.strip()
    if not s:
        return "", name
    try:
        if s.startswith("["):
            idx = s.find("]")
            if idx > 0:
                prefix = s[1:idx].strip()
                rest = s[idx + 1:].strip()
                return prefix, rest if rest else s
        return "", s
    except Exception:
        return "", s


def _estimate_tokens(text: str) -> int:
    """∆Ø·ªõc l∆∞·ª£ng s·ªë token (~4 k√Ω t·ª±/token)."""
    if not text:
        return 0
    return max(1, len(text) // 4)


# Tr·ªçng s·ªë khi re-rank c√≥ prefix: vector 0.55, recency 0.1, bias 0.2, prefix 0.15
PREFIX_WEIGHT = 0.15
VECTOR_WEIGHT_WITH_PREFIX = 0.55
RECENCY_WEIGHT_UNCHANGED = 0.1
IMPORTANCE_WEIGHT_UNCHANGED = 0.2


def get_prefix_key_from_entity_name(entity_name: str) -> str:
    """L·∫•y prefix_key (vi·∫øt HOA, kh√¥ng ngo·∫∑c) t·ª´ entity_name. VD: '[CHARACTER] John' -> 'CHARACTER'."""
    if not entity_name or not isinstance(entity_name, str):
        return "OTHER"
    prefix, _ = extract_prefix(entity_name.strip())
    return (prefix or "OTHER").strip().upper().replace(" ", "_") or "OTHER"


def _rerank_by_score_with_prefix(
    rows: List[Dict],
    top_k: int,
    inferred_prefixes: Optional[List[str]] = None,
) -> List[Dict]:
    """Re-rank v·ªõi bonus cho entry c√≥ prefix n·∫±m trong inferred_prefixes. D√πng khi Router tr·∫£ v·ªÅ inferred_prefixes."""
    if not inferred_prefixes:
        return _rerank_by_score(rows, top_k)
    normalized_inferred = {str(p).strip().upper().replace(" ", "_") for p in inferred_prefixes if p}
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        pk = get_prefix_key_from_entity_name(item.get("entity_name") or "")
        prefix_bonus = 1.0 if pk in normalized_inferred else 0.0
        item["_final_score"] = (
            (vector_sim * VECTOR_WEIGHT_WITH_PREFIX)
            + (recency * RECENCY_WEIGHT_UNCHANGED)
            + (importance * IMPORTANCE_WEIGHT_UNCHANGED)
            + (prefix_bonus * PREFIX_WEIGHT)
        )
    sorted_rows = sorted(rows, key=lambda x: x.get("_final_score", 0.0), reverse=True)
    for item in sorted_rows:
        item.pop("_final_score", None)
    return sorted_rows[:top_k]


def _get_prefix_section_order_and_labels() -> Tuple[List[str], Dict[str, str]]:
    """L·∫•y th·ª© t·ª± v√† nh√£n section t·ª´ DB (Config.get_prefix_setup()). Tr·∫£ v·ªÅ (order, label_map)."""
    setup = Config.get_prefix_setup()
    order = []
    labels: Dict[str, str] = {}
    for p in setup:
        pk = (p.get("prefix_key") or "").strip().upper().replace(" ", "_")
        if pk:
            order.append(pk)
            labels[pk] = pk
    return order, labels


def format_bible_context_by_sections(raw_list: List[Dict]) -> str:
    """Gom k·∫øt qu·∫£ Bible theo section theo prefix; th·ª© t·ª± v√† nh√£n l·∫•y t·ª´ DB (get_prefix_setup)."""
    if not raw_list:
        return ""
    grouped: Dict[str, List[Dict]] = defaultdict(list)
    for item in raw_list:
        pk = get_prefix_key_from_entity_name(item.get("entity_name") or "")
        grouped[pk].append(item)
    order, labels = _get_prefix_section_order_and_labels()
    seen = set(order)
    for pk in grouped:
        if pk not in seen:
            order.append(pk)
            if pk not in labels:
                labels[pk] = pk
    sections = []
    for pk in order:
        items = grouped.get(pk, [])
        if not items:
            continue
        label = labels.get(pk, pk)
        block = "\n".join(
            f"- [{e.get('entity_name', '')}]: {e.get('description', '')}"
            for e in items
        )
        sections.append(f"\n--- {label} ---\n{block}")
    return "\n".join(sections).strip()


def get_bible_index(story_id: str, max_tokens: int = 2000) -> str:
    """
    Danh s√°ch th√¥ cho Router: m·ªói d√≤ng "Entity: [LO·∫†I] T√™n" (gi·ªØ nguy√™n format [PREFIX] Name).
    Top 100 theo (lookup_count + importance_bias). C√≥ parent_id th√¨ g·ª£i √Ω th·ª±c th·ªÉ g·ªëc.
    """
    if not story_id:
        return ""
    try:
        services = init_services()
        if not services:
            return ""
        supabase = services["supabase"]
        try:
            rows = (
                supabase.table("story_bible")
                .select("entity_name, lookup_count, importance_bias, parent_id")
                .eq("story_id", story_id)
                .execute()
            )
        except Exception:
            try:
                rows = (
                    supabase.table("story_bible")
                    .select("entity_name, lookup_count, importance_bias")
                    .eq("story_id", story_id)
                    .execute()
                )
            except Exception:
                return ""
        data = list(rows.data) if rows.data else []
        for r in data:
            r.setdefault("parent_id", None)
        def _score(r):
            try:
                lk = int(r.get("lookup_count") or 0)
                bi = r.get("importance_bias")
                b = float(bi) if bi is not None else 0.0
                return lk + b
            except (TypeError, ValueError):
                return 0
        data.sort(key=_score, reverse=True)
        top100 = data[:100]
        parent_ids = [r["parent_id"] for r in top100 if r.get("parent_id")]
        parent_names: Dict[Any, str] = {}
        if parent_ids:
            try:
                ids = list(set(str(pid) for pid in parent_ids if pid is not None))
                if ids:
                    pr = supabase.table("story_bible").select("id, entity_name").in_("id", ids).execute()
                    if pr.data:
                        for row in pr.data:
                            try:
                                _, disp = extract_prefix(row.get("entity_name") or "")
                                parent_names[row.get("id")] = disp.strip() or "(g·ªëc)"
                            except Exception:
                                parent_names[row.get("id")] = (row.get("entity_name") or "").strip() or "(g·ªëc)"
            except Exception:
                pass
        lines = []
        for r in top100:
            name = r.get("entity_name")
            if not name:
                continue
            line = f"Entity: {name}"
            pid = r.get("parent_id")
            if pid is not None and parent_names.get(pid):
                line += f" (g·ªëc: {parent_names[pid]})"
            lines.append(line)
        out = "\n".join(lines) if lines else ""
        if _estimate_tokens(out) > max_tokens:
            out = out[: max(100, max_tokens * 4)]
        return out
    except Exception as e:
        print(f"get_bible_index error: {e}")
        return ""


def get_bible_entries(story_id: str) -> List[Dict[str, Any]]:
    """Tr·∫£ v·ªÅ danh s√°ch entity trong Bible c·ªßa story: [{id, entity_name}, ...]. ƒê·ªÉ resolve t√™n -> id khi ƒë·ªÅ xu·∫•t quan h·ªá."""
    if not story_id:
        return []
    try:
        services = init_services()
        if not services:
            return []
        services = init_services()
        supabase = services["supabase"] if services else None
        if not supabase:
            return []
        r = (
            supabase
            .table("story_bible")
            .select("id, entity_name")
            .eq("story_id", story_id)
            .execute()
        )
        return list(r.data) if r.data else []
    except Exception:
        return []


def suggest_relations(content: str, story_id: str) -> List[Dict[str, Any]]:
    """
    AI qu√©t n·ªôi dung (ch∆∞∆°ng/ƒëo·∫°n) v√† so kh·ªõp v·ªõi bible_index ƒë·ªÉ ƒë·ªÅ xu·∫•t:
    - Quan h·ªá gi·ªØa hai th·ª±c th·ªÉ: Source, Target, Relation_Type, Reason -> tr·∫£ v·ªÅ kind="relation".
    - Nh√¢n v·∫≠t ti·∫øn h√≥a (1-n): th·ª±c th·ªÉ m·ªõi c√πng g·ªëc -> g·ª£i √Ω parent_id, kind="parent".
    Output: list of {
      "kind": "relation" | "parent",
      "source_entity_id", "target_entity_id", "relation_type", "description" (reason), "story_id"  (cho relation),
      ho·∫∑c "entity_id", "parent_entity_id", "reason" (cho parent).
    }
    """
    if not content or not content.strip() or not story_id:
        return []
    entries = get_bible_entries(story_id)
    if not entries:
        return []
    name_to_id = {}
    for e in entries:
        name = (e.get("entity_name") or "").strip()
        if name:
            name_to_id[name] = e.get("id")
    index_text = "\n".join([f"- {e.get('entity_name', '')}" for e in entries[:150]])
    prompt = f"""B·∫°n l√† tr·ª£ l√Ω ph√¢n t√≠ch vƒÉn b·∫£n. Cho N·ªòI DUNG v√† DANH S√ÅCH TH·ª∞C TH·ªÇ (Bible) c·ªßa m·ªôt truy·ªán.

DANH S√ÅCH TH·ª∞C TH·ªÇ (ch√≠nh x√°c t·ª´ Bible):
{index_text}

N·ªòI DUNG (ƒëo·∫°n/ch∆∞∆°ng c·∫ßn ph√¢n t√≠ch):
---
{content[:15000]}
---

Nhi·ªám v·ª•:
1) QUAN H·ªÜ: T√¨m c√°c c·∫∑p th·ª±c th·ªÉ c√≥ t∆∞∆°ng t√°c/li√™n quan trong n·ªôi dung (v√≠ d·ª•: A l√† b·∫°n c·ªßa B, X ph·∫£n b·ªôi Y). V·ªõi m·ªói c·∫∑p, tr·∫£ v·ªÅ source (t√™n ƒë√∫ng nh∆∞ trong danh s√°ch), target, relation_type (ng·∫Øn g·ªçn: b·∫°n, k·∫ª th√π, ƒë·ªìng ƒë·ªôi, y√™u, cha-con...), reason (l√Ω do ng·∫Øn).
2) NH√ÇN V·∫¨T TI·∫æN H√ìA (1-n): N·∫øu trong n·ªôi dung c√≥ th·ª±c th·ªÉ m·ªõi m√† th·ª±c ch·∫•t l√† "phi√™n b·∫£n kh√°c" c·ªßa m·ªôt th·ª±c th·ªÉ ƒë√£ c√≥ (VD: "C∆∞·ªùng l√∫c nh·ªè" / "C∆∞·ªùng l√∫c l·ªõn", c√πng m·ªôt nh√¢n v·∫≠t ·ªü hai giai ƒëo·∫°n), KH√îNG t·∫°o quan h·ªá r·ªùi r·∫°c m√† g·ª£i √Ω ƒë·∫∑t parent: entity (t√™n th·ª±c th·ªÉ con/bi·∫øn th·ªÉ) v√† parent (t√™n th·ª±c th·ªÉ g·ªëc trong danh s√°ch), k√®m reason.

Tr·∫£ v·ªÅ ƒê√öNG m·ªôt JSON object v·ªõi hai key:
- "relations": [ {{ "source": "<t√™n trong danh s√°ch>", "target": "<t√™n trong danh s√°ch>", "relation_type": "...", "reason": "..." }} ]
- "parent_suggestions": [ {{ "entity": "<t√™n con/bi·∫øn th·ªÉ trong danh s√°ch>", "parent": "<t√™n g·ªëc trong danh s√°ch>", "reason": "..." }} ]

Ch·ªâ d√πng t√™n c√≥ trong DANH S√ÅCH TH·ª∞C TH·ªÇ. N·∫øu kh√¥ng c√≥ g√¨ ph√π h·ª£p, tr·∫£ v·ªÅ "relations": [] v√† "parent_suggestions": [].
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m."""

    try:
        response = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=Config.ROUTER_MODEL,
            temperature=0.2,
            max_tokens=2000,
        )
        text = (response.choices[0].message.content or "").strip()
        text = re.sub(r"^```\w*\n?", "", text).strip()
        text = re.sub(r"\n?```\s*$", "", text).strip()
        data = json.loads(text)
        relations_in = data.get("relations") or []
        parent_in = data.get("parent_suggestions") or []

        def resolve_name(name: str) -> Optional[Any]:
            n = (name or "").strip()
            if n in name_to_id:
                return name_to_id[n]
            for k, vid in name_to_id.items():
                if n in k or k in n:
                    return vid
            return None

        out = []
        for r in relations_in:
            src_id = resolve_name(r.get("source") or "")
            tgt_id = resolve_name(r.get("target") or "")
            if src_id and tgt_id and src_id != tgt_id:
                out.append({
                    "kind": "relation",
                    "source_entity_id": src_id,
                    "target_entity_id": tgt_id,
                    "relation_type": (r.get("relation_type") or "li√™n quan").strip(),
                    "description": (r.get("reason") or "").strip(),
                    "story_id": story_id,
                })
        for p in parent_in:
            child_id = resolve_name(p.get("entity") or "")
            parent_id = resolve_name(p.get("parent") or "")
            if child_id and parent_id and child_id != parent_id:
                out.append({
                    "kind": "parent",
                    "entity_id": child_id,
                    "parent_entity_id": parent_id,
                    "reason": (p.get("reason") or "").strip(),
                })
        return out
    except Exception as e:
        print(f"suggest_relations error: {e}")
        return []


class SmartAIRouter:
    """B·ªô ƒë·ªãnh tuy·∫øn AI th√¥ng minh v·ªõi hybrid search v√† bible index"""

    @staticmethod
    def ai_router_pro_v2(user_prompt: str, chat_history_text: str, project_id: str = None) -> Dict:
        """Router V2: Ph√¢n t√≠ch Intent v√† Target Files, c√≥ inject bible_index ƒë·ªÉ nh·∫≠n di·ªán √Ω ƒë·ªãnh."""
        rules_context = ""
        bible_index = ""
        prefix_setup_str = ""
        if project_id:
            rules_context = ContextManager.get_mandatory_rules(project_id)
            bible_index = get_bible_index(project_id, max_tokens=2000)
        try:
            prefix_setup = Config.get_prefix_setup()
            if prefix_setup:
                prefix_setup_str = "\n".join(
                    f"- [{p.get('prefix_key', '')}]: {p.get('description', '')}" for p in prefix_setup
                )
            else:
                prefix_setup_str = "(Ch∆∞a c·∫•u h√¨nh lo·∫°i th·ª±c th·ªÉ trong Bible Prefix / b·∫£ng bible_prefix_config.)"
        except Exception:
            prefix_setup_str = "(Ch∆∞a c·∫•u h√¨nh lo·∫°i th·ª±c th·ªÉ trong Bible Prefix.)"

        router_prompt = f"""
        ƒê√≥ng vai ƒêi·ªÅu Ph·ªëi Vi√™n D·ª± √Ån (Project Coordinator).
        
        ‚ö†Ô∏è QUY T·∫ÆC B·∫ÆT BU·ªòC:
        {rules_context}

        B·∫¢NG M√î T·∫¢ C√ÅC LO·∫†I TH·ª∞C TH·ªÇ (do ng∆∞·ªùi d√πng cung c·∫•p):
        {prefix_setup_str}

        DANH S√ÅCH TH·ª∞C TH·ªÇ TRONG STORY BIBLE (m·ªói d√≤ng: Entity: [LO·∫†I] T√™n):
        {bible_index if bible_index else "(Ch∆∞a c√≥ d·ªØ li·ªáu)"}

        Y√äU C·∫¶U ƒêI·ªÄU H∆Ø·ªöNG: D·ª±a v√†o b·∫£ng m√¥ t·∫£ c√°c lo·∫°i th·ª±c th·ªÉ. N·∫øu user h·ªèi v·ªÅ th·ª±c th·ªÉ (nh√¢n v·∫≠t, ƒë·ªãa ƒëi·ªÉm...) -> search_bible. N·∫øu user h·ªèi theo t·ª´ng ƒëo·∫°n, t·ª´ng ph·∫ßn, n·ªôi dung chi ti·∫øt trong ch∆∞∆°ng/file ƒë√£ chunk (Data Analyze / Excel/Word chunk) -> search_chunks. N·∫øu user h·ªèi di·ªÖn bi·∫øn, s·ª± ki·ªán theo th·ªùi gian ho·∫∑c n·ªôi dung ch∆∞∆°ng ƒë·∫ßy ƒë·ªß -> read_full_content ho·∫∑c chapter_range. Ch·ªâ ∆∞u ti√™n search_chapters khi user h·ªèi r√µ v·ªÅ di·ªÖn bi·∫øn, s·ª± ki·ªán theo th·ªùi gian ho·∫∑c n·ªôi dung ch∆∞∆°ng c·ª• th·ªÉ.

        L·ªäCH S·ª¨ CHAT:
        {chat_history_text}
        
        INPUT C·ª¶A USER: "{user_prompt}"
        
        NHI·ªÜM V·ª§: Ph√¢n t√≠ch intent, target files V√Ä nh·∫≠n di·ªán PH·∫†M VI CH∆Ø∆†NG (Chapter Range) n·∫øu user ƒë·ªÅ c·∫≠p.

        PH√ÇN LO·∫†I INTENT:
        1. "numerical_calculation": User h·ªèi v·ªÅ S·ªê LI·ªÜU, t√≠nh to√°n, th·ªëng k√™ (t·ªïng, trung b√¨nh, ƒë·∫øm, %, doanh thu, chi ph√≠...) -> ∆Øu ti√™n Python Executor v·ªõi Pandas/NumPy.
        2. "read_full_content": User mu·ªën S·ª≠a, Review, Vi·∫øt ti·∫øp, Ki·ªÉm tra code/vƒÉn, ho·∫∑c nh·∫Øc ƒë·∫øn t√™n file c·ª• th·ªÉ -> C·∫ßn ƒë·ªçc NGUY√äN VƒÇN FILE.
        3. "search_chunks": User h·ªèi th√¥ng tin chi ti·∫øt theo t·ª´ng ƒëo·∫°n/ph·∫ßn; d·ªØ li·ªáu ƒë√£ chunk (Excel theo d√≤ng, Word/theo ch∆∞∆°ng t·ª´ Data Analyze); c·∫ßn tr√≠ch ƒëo·∫°n c·ª• th·ªÉ, n·ªôi dung t·ª´ng ph·∫ßn, ho·∫∑c t√¨m trong c√°c chunk ƒë√£ vector h√≥a -> Tra chunks (vector + reverse lookup chapter/arc). ∆Øu ti√™n search_chunks khi c√¢u h·ªèi c·∫ßn tr√≠ch ƒëo·∫°n c·ª• th·ªÉ ho·∫∑c d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chunk.
        4. "search_bible": User h·ªèi th√¥ng tin chung, Lore, c·ªët truy·ªán, quy ƒë·ªãnh, kh√°i ni·ªám, ho·∫∑c nh·∫Øc t√™n nh√¢n v·∫≠t/th·ª±c th·ªÉ c√≥ trong danh s√°ch Bible tr√™n -> Tra c·ª©u Bible (search_bible / get_entity_relations).
        5. "chat_casual": Ch√†o h·ªèi, khen ch√™, n√≥i chuy·ªán phi·∫øm kh√¥ng c·∫ßn d·ªØ li·ªáu d·ª± √°n.
        6. "mixed_context": C·∫ßn c·∫£ n·ªôi dung file V√Ä ki·∫øn th·ª©c Bible.

        inferred_prefixes: Khi intent l√† search_bible ho·∫∑c mixed_context, ƒëi·ªÅn m·∫£ng prefix_key (t·ª´ B·∫¢NG M√î T·∫¢ tr√™n) t∆∞∆°ng ·ª©ng lo·∫°i th·ª±c th·ªÉ user ƒëang h·ªèi. VD: h·ªèi nh√¢n v·∫≠t -> ["CHARACTER"]; h·ªèi ƒë·ªãa ƒëi·ªÉm -> ["LOCATION"]; h·ªèi lore + nh√¢n v·∫≠t -> ["LORE", "CHARACTER"]. Vi·∫øt HOA, kh√¥ng ngo·∫∑c. N·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c -> [].

        NH·∫¨N DI·ªÜN PH·∫†M VI CH∆Ø∆†NG (chapter_range):
        - N·∫øu user n√≥i "ch∆∞∆°ng ƒë·∫ßu", "m·∫•y ch∆∞∆°ng ƒë·∫ßu", "ƒë·∫ßu truy·ªán" -> ƒë·∫∑t "chapter_range_mode": "first", "chapter_range_count": 5 (ho·∫∑c s·ªë user n√≥i n·∫øu r√µ).
        - N·∫øu user n√≥i "m·ªõi nh·∫•t", "ch∆∞∆°ng m·ªõi", "m·∫•y ch∆∞∆°ng cu·ªëi" -> ƒë·∫∑t "chapter_range_mode": "latest", "chapter_range_count": 5 (ho·∫∑c s·ªë user n√≥i n·∫øu r√µ).
        - N·∫øu user n√≥i c·ª• th·ªÉ "t·ª´ ch∆∞∆°ng 5 ƒë·∫øn 10", "ch∆∞∆°ng 5 ƒë·∫øn 10" -> ƒë·∫∑t "chapter_range": [5, 10], "chapter_range_mode": "range".
        - N·∫øu kh√¥ng li√™n quan ph·∫°m vi ch∆∞∆°ng -> ƒë·ªÉ "chapter_range": null, "chapter_range_mode": null.

        OUTPUT (JSON ONLY):
        {{
            "intent": "numerical_calculation" | "read_full_content" | "search_chunks" | "search_bible" | "chat_casual" | "mixed_context",
            "target_files": ["t√™n file 1", "t√™n file 2"],
            "target_bible_entities": ["t√™n th·ª±c th·ªÉ 1", "t√™n th·ª±c th·ªÉ 2"],
            "inferred_prefixes": ["CHARACTER", "LOCATION"],
            "reason": "L√Ω do ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát",
            "rewritten_query": "Vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa user cho r√µ nghƒ©a h∆°n ƒë·ªÉ search database",
            "chapter_range": [start, end] ho·∫∑c null,
            "chapter_range_mode": "first" | "latest" | "range" | null,
            "chapter_range_count": 5
        }}
        """

        messages = [
            {"role": "system", "content": "B·∫°n l√† AI Router th√¥ng minh. Ch·ªâ tr·∫£ v·ªÅ JSON."},
            {"role": "user", "content": router_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.1,
                max_tokens=500,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            content = AIService.clean_json_text(content)

            result = json.loads(content)

            result.setdefault("target_files", [])
            result.setdefault("target_bible_entities", [])
            result.setdefault("inferred_prefixes", [])
            result.setdefault("rewritten_query", user_prompt)
            result.setdefault("chapter_range", None)
            result.setdefault("chapter_range_mode", None)
            result.setdefault("chapter_range_count", 5)
            if not isinstance(result.get("inferred_prefixes"), list):
                result["inferred_prefixes"] = []
            # Ch·ªâ gi·ªØ inferred_prefixes c√≥ trong DB (get_valid_prefix_keys)
            valid_keys = Config.get_valid_prefix_keys()
            if valid_keys:
                result["inferred_prefixes"] = [
                    p for p in result["inferred_prefixes"]
                    if p and str(p).strip().upper().replace(" ", "_") in valid_keys
                ]

            return result

        except Exception as e:
            print(f"Router error: {e}")
            return {
                "intent": "chat_casual",
                "target_files": [],
                "target_bible_entities": [],
                "inferred_prefixes": [],
                "reason": f"Router error: {e}",
                "rewritten_query": user_prompt,
                "chapter_range": None,
                "chapter_range_mode": None,
                "chapter_range_count": 5,
            }


# ==========================================
# üìö CONTEXT MANAGER (V5 + V6 Arc & Reverse Lookup)
# ==========================================
class ContextManager:
    """Qu·∫£n l√Ω context cho AI v·ªõi kh·∫£ nƒÉng k·∫øt h·ª£p nhi·ªÅu ngu·ªìn. V6: Arc scoping + Triangle assembler."""

    @staticmethod
    def _build_arc_scope_context(project_id: str, current_arc_id: Optional[str], session_state: Optional[Dict] = None) -> Tuple[str, int]:
        """
        V6 MODULE 1 & 3: Build [Past Arc Summaries] + [Current Arc] for Sequential/Standalone.
        Global Bible is still injected via get_mandatory_rules and search_bible below.
        Returns (text, estimated_tokens).
        """
        if not ArcService or not current_arc_id:
            return "", 0
        arc = ArcService.get_arc(current_arc_id)
        if not arc:
            return "", 0
        parts = []
        scope = ArcService.get_scope_for_search(project_id, current_arc_id)
        if scope.get("scope_type") == ArcService.ARC_TYPE_SEQUENTIAL and scope.get("arc_summaries"):
            parts.append("[PAST ARC SUMMARIES - Timeline Inheritance]")
            for a in scope["arc_summaries"]:
                parts.append("- ARC: %s\n  Summary: %s" % (a.get("name", ""), (a.get("summary") or "").strip() or "(none)"))
            parts.append("")
        parts.append("[MACRO CONTEXT - ARC: %s]" % (arc.get("name") or "Current"))
        parts.append("Summary: %s" % ((arc.get("summary") or "").strip() or "(none)"))
        text = "\n".join(parts)
        return text, AIService.estimate_tokens(text)

    @staticmethod
    def build_context_with_chunk_reverse_lookup(
        project_id: str,
        chunk_ids: List[str],
        current_arc_id: Optional[str],
        token_limit: int = 12000,
    ) -> Tuple[str, List[str], int]:
        """
        V6 MODULE 3: Assemble context from chunk IDs using Triangle (Macro/Meso/Micro).
        Optionally prepend arc scope. Returns (full_context, sources, total_tokens).
        """
        context_parts = []
        sources = []
        total_tokens = 0
        if ArcService and current_arc_id:
            arc_scope, t = ContextManager._build_arc_scope_context(project_id, current_arc_id, None)
            if arc_scope:
                context_parts.append(arc_scope)
                total_tokens += t
        if ReverseLookupAssembler and chunk_ids:
            assembled, chunk_sources = ReverseLookupAssembler.assemble_from_chunks(chunk_ids, token_limit=token_limit)
            if assembled:
                context_parts.append("[REVERSE LOOKUP - Micro to Macro Evidence]\n" + assembled)
                total_tokens += AIService.estimate_tokens(assembled)
                sources.extend(chunk_sources)
        return "\n\n".join(context_parts), sources, total_tokens

    @staticmethod
    def get_entity_relations(entity_id: Any, project_id: str) -> str:
        """L·∫•y quan h·ªá c·ªßa entity: t·ª´ b·∫£ng entity_relations (n·∫øu c√≥) v√† c√°c bi·∫øn th·ªÉ (parent_id) t·ª´ story_bible. Tr·∫£ v·ªÅ chu·ªói d·∫°ng '> [RELATION]: ...'. Defensive: kh√¥ng crash n·∫øu b·∫£ng/ c·ªôt ch∆∞a c√≥."""
        lines = []
        try:
            services = init_services()
            if not services:
                return ""
            supabase = services["supabase"]

            try:
                rel_res = supabase.table("entity_relations").select("*").or_(
                    f"source_entity_id.eq.{entity_id},target_entity_id.eq.{entity_id}"
                ).execute()
            except Exception:
                try:
                    rel_res = supabase.table("entity_relations").select("*").or_(
                        f"entity_id.eq.{entity_id},target_entity_id.eq.{entity_id}"
                    ).execute()
                except Exception:
                    rel_res = None
            if rel_res:
                if rel_res.data:
                    id_to_name = {}
                    for r in rel_res.data:
                        eid = r.get("entity_id") or r.get("source_entity_id") or r.get("from_entity_id")
                        tid = r.get("target_entity_id") or r.get("to_entity_id")
                        if eid and eid not in id_to_name:
                            id_to_name[eid] = None
                        if tid and tid not in id_to_name:
                            id_to_name[tid] = None
                    if id_to_name:
                        sb = supabase.table("story_bible").select("id, entity_name").eq(
                            "story_id", project_id
                        ).in_("id", list(id_to_name.keys())).execute()
                        if sb.data:
                            for row in sb.data:
                                id_to_name[row.get("id")] = row.get("entity_name") or ""
                    for r in rel_res.data:
                        rel_type = r.get("relation_type") or r.get("relation") or "li√™n quan"
                        eid = r.get("entity_id") or r.get("source_entity_id") or r.get("from_entity_id")
                        tid = r.get("target_entity_id") or r.get("to_entity_id")
                        name_a = id_to_name.get(eid) if eid else ""
                        name_b = id_to_name.get(tid) if tid else ""
                        if name_a or name_b:
                            lines.append(f"> [RELATION]: {name_a or 'Entity'} l√† {rel_type} c·ªßa {name_b or 'Entity'}.")

            try:
                variants = supabase.table("story_bible").select("entity_name, description").eq(
                    "story_id", project_id
                ).eq("parent_id", entity_id).execute()
                if variants.data:
                    for v in variants.data:
                        name = v.get("entity_name") or ""
                        desc = (v.get("description") or "")[:200]
                        if name:
                            lines.append(f"> [RELATION]: Bi·∫øn th·ªÉ: {name} ‚Äî {desc}...")
            except Exception:
                pass
        except Exception as e:
            print(f"get_entity_relations error: {e}")
        return "\n".join(lines) if lines else ""

    # Gi·ªõi h·∫°n token khi load nhi·ªÅu ch∆∞∆°ng (∆∞u ti√™n summary n·∫øu v∆∞·ª£t)
    DEFAULT_CHAPTER_TOKEN_LIMIT = 60000

    @staticmethod
    def _resolve_chapter_range(
        project_id: str,
        chapter_range_mode: Optional[str],
        chapter_range_count: int,
        chapter_range: Optional[List[int]],
    ) -> Optional[Tuple[int, int]]:
        """Tr·∫£ v·ªÅ (start, end) chapter_number t·ª´ router. first/latest query DB; range d√πng tr·ª±c ti·∫øp."""
        try:
            services = init_services()
            if not services:
                return None
            supabase = services["supabase"]
            count = max(1, min(50, int(chapter_range_count) if chapter_range_count else 5))

            if chapter_range_mode == "range" and chapter_range and len(chapter_range) >= 2:
                return (int(chapter_range[0]), int(chapter_range[1]))

            if chapter_range_mode == "first":
                r = supabase.table("chapters").select("chapter_number").eq(
                    "story_id", project_id
                ).order("chapter_number").limit(1).execute()
                if r.data and len(r.data) > 0:
                    start = int(r.data[0].get("chapter_number", 1))
                    return (start, start + count - 1)
                return (1, count)

            if chapter_range_mode == "latest":
                r = supabase.table("chapters").select("chapter_number").eq(
                    "story_id", project_id
                ).order("chapter_number", desc=True).limit(1).execute()
                if r.data and len(r.data) > 0:
                    end = int(r.data[0].get("chapter_number", 1))
                    start = max(1, end - count + 1)
                    return (start, end)
                return (1, count)

        except Exception as e:
            print(f"_resolve_chapter_range error: {e}")
        return None

    @staticmethod
    def load_chapters_by_range(
        project_id: str,
        start: int,
        end: int,
        token_limit: int = 60000,
    ) -> Tuple[str, List[str]]:
        """Load ch∆∞∆°ng theo kho·∫£ng chapter_number; c√≥ summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary cho ch∆∞∆°ng c≈©, full content cho ch∆∞∆°ng ƒëang b√†n (cu·ªëi)."""
        try:
            services = init_services()
            if not services:
                return "", []
            supabase = services["supabase"]
            r = supabase.table("chapters").select("*").eq(
                "story_id", project_id
            ).gte("chapter_number", start).lte("chapter_number", end).order(
                "chapter_number"
            ).execute()
            rows = r.data if r.data else []
        except Exception as e:
            print(f"load_chapters_by_range error: {e}")
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        focus_idx = len(rows) - 1 if rows else -1

        for i, item in enumerate(rows):
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number', i+1)}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            use_full = (token_limit <= 0 or total_tokens < token_limit) or (i == focus_idx)
            block = f"\n\n=== üìÑ {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary and not use_full:
                block += f"(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def load_full_content(
        file_names: List[str],
        project_id: str,
        token_limit: int = 60000,
        focus_chapter_name: Optional[str] = None,
    ) -> Tuple[str, List[str]]:
        """Load n·ªôi dung file/ch∆∞∆°ng; th√™m summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary, full content cho ch∆∞∆°ng focus."""
        if not file_names:
            return "", []

        try:
            services = init_services()
            supabase = services["supabase"]
        except Exception:
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        rows_with_meta = []

        for name in file_names:
            try:
                res = supabase.table("chapters").select("*").eq(
                    "story_id", project_id
                ).ilike("title", f"%{name}%").execute()
            except Exception:
                res = type("Res", (), {"data": None})()

            if res.data and len(res.data) > 0:
                item = res.data[0]
                item["_name"] = name
                item["_is_focus"] = (focus_chapter_name and focus_chapter_name in (item.get("title") or ""))
                rows_with_meta.append(item)
            else:
                try:
                    res_bible = supabase.table("story_bible").select(
                        "entity_name, description"
                    ).eq("story_id", project_id).ilike("entity_name", f"%{name}%").execute()
                    if res_bible.data and len(res_bible.data) > 0:
                        item = res_bible.data[0]
                        full_text += f"\n\n=== ‚ö†Ô∏è BIBLE SUMMARY: {item.get('entity_name', name)} ===\n{item.get('description', '')}\n"
                        loaded_sources.append(f"üóÇÔ∏è {item.get('entity_name', name)} (Summary)")
                except Exception:
                    pass

        for item in rows_with_meta:
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number')}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            is_focus = item.get("_is_focus", False)
            use_full = token_limit <= 0 or total_tokens + AIService.estimate_tokens(content) <= token_limit or is_focus
            block = f"\n\n=== üìÑ SOURCE FILE/CHAP: {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary:
                block += "(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def get_mandatory_rules(project_id: str) -> str:
        """L·∫•y t·∫•t c·∫£ c√°c lu·∫≠t (RULE) b·∫Øt bu·ªôc"""
        try:
            services = init_services()
            supabase = services['supabase']

            res = supabase.table("story_bible") \
                .select("description") \
                .eq("story_id", project_id) \
                .ilike("entity_name", "%[RULE]%") \
                .execute()

            if res.data:
                rules_text = "\n".join([f"- {r['description']}" for r in res.data])
                return f"\nüî• --- MANDATORY RULES ---\n{rules_text}\n"
            return ""
        except Exception as e:
            print(f"Error getting rules: {e}")
            return ""

    @staticmethod
    def build_context(
        router_result: Dict,
        project_id: str,
        persona: Dict,
        strict_mode: bool = False,
        current_arc_id: Optional[str] = None,
        session_state: Optional[Dict] = None,
        free_chat_mode: bool = False,
        max_context_tokens: Optional[int] = None,
    ) -> Tuple[str, List[str], int]:
        """X√¢y d·ª±ng context t·ª´ router result. max_context_tokens: gi·ªõi h·∫°n ƒë·ªô d√†i (t·ª´ Settings Context Size); None = kh√¥ng gi·ªõi h·∫°n."""
        context_parts = []
        sources = []
        total_tokens = 0

        persona_text = f"üé≠ PERSONA: {persona['role']}\n{persona['core_instruction']}\n"
        context_parts.append(persona_text)
        total_tokens += AIService.estimate_tokens(persona_text)

        if free_chat_mode:
            rules_text = ContextManager.get_mandatory_rules(project_id)
            if rules_text:
                context_parts.append(rules_text)
                total_tokens += AIService.estimate_tokens(rules_text)
            free_instruction = "[CH·∫æ ƒê·ªò CHAT T·ª∞ DO / CHAT PHI·∫æM]\nTr·∫£ l·ªùi nh∆∞ chatbot th√¥ng th∆∞·ªùng, d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t. Kh√¥ng b·∫Øt bu·ªôc d·ª±a v√†o d·ªØ li·ªáu d·ª± √°n (Bible/chunk/file); c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi ch·ªß ƒë·ªÅ."
            context_parts.append(free_instruction)
            total_tokens += AIService.estimate_tokens(free_instruction)
            sources.append("üåê Chat t·ª± do")
            return "\n".join(context_parts), sources, total_tokens

        # V6 MODULE 1: Arc scope (Past Arc Summaries + Current Arc)
        if current_arc_id and ArcService:
            arc_scope, arc_tokens = ContextManager._build_arc_scope_context(project_id, current_arc_id, session_state)
            if arc_scope:
                context_parts.append(arc_scope)
                total_tokens += arc_tokens
                sources.append("üìê Arc Scope")

        if strict_mode:
            strict_text = """
            \n\n‚ÄºÔ∏è CH·∫æ ƒê·ªò NGHI√äM NG·∫∂T (STRICT MODE) ƒêANG B·∫¨T:
            1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong [CONTEXT].
            2. TUY·ªÜT ƒê·ªêI KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c d√πng ki·∫øn th·ª©c b√™n ngo√†i ƒë·ªÉ ƒëi·ªÅn v√†o ch·ªó tr·ªëng.
            3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong Context, h√£y tr·∫£ l·ªùi: "D·ªØ li·ªáu d·ª± √°n ch∆∞a c√≥ th√¥ng tin n√†y."
            4. N·∫øu User h·ªèi v·ªÅ "l·ªãch s·ª≠", "c·ªët truy·ªán", h√£y ∆∞u ti√™n tr√≠ch xu·∫•t t·ª´ [KNOWLEDGE BASE].
            5. Kh√¥ng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c d·ªØ li·ªáu th·ª±c t·∫ø (fact) ch·ªâ v√¨ t√≠nh c√°ch Persona.
            """
            context_parts.append(strict_text)
            total_tokens += AIService.estimate_tokens(strict_text)

        rules_text = ContextManager.get_mandatory_rules(project_id)
        if rules_text:
            context_parts.append(rules_text)
            total_tokens += AIService.estimate_tokens(rules_text)

        intent = router_result.get("intent", "chat_casual")
        target_files = router_result.get("target_files", [])
        target_bible_entities = router_result.get("target_bible_entities", [])
        chapter_range_mode = router_result.get("chapter_range_mode")
        chapter_range_count = router_result.get("chapter_range_count", 5)
        chapter_range = router_result.get("chapter_range")

        if intent == "read_full_content":
            full_text, source_names = "", []
            range_bounds = ContextManager._resolve_chapter_range(
                project_id, chapter_range_mode, chapter_range_count, chapter_range
            )
            if range_bounds is not None:
                full_text, source_names = ContextManager.load_chapters_by_range(
                    project_id, range_bounds[0], range_bounds[1],
                    token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                )
            if not full_text and target_files:
                full_text, source_names = ContextManager.load_full_content(
                    target_files, project_id,
                    token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                )
            if full_text:
                context_parts.append(f"\n--- TARGET CONTENT ---\n{full_text}")
                sources.extend(source_names)
                total_tokens += AIService.estimate_tokens(full_text)

        elif intent == "search_chunks":
            # Chunk vector search + reverse lookup (chunk -> chapter -> arc)
            chunk_ids = []
            query_for_chunk = (router_result.get("rewritten_query") or (router_result.get("target_files") or [""])[0] or "").strip()
            chunk_rows = search_chunks_vector(
                query_for_chunk or "n·ªôi dung",
                project_id,
                arc_id=current_arc_id,
                top_k=8,
            )
            if chunk_rows:
                chunk_ids = [str(c.get("id")) for c in chunk_rows if c.get("id")]
            if not chunk_ids and current_arc_id and query_for_chunk:
                chunk_rows = search_chunks_vector(query_for_chunk, project_id, arc_id=None, top_k=8)
                if chunk_rows:
                    chunk_ids = [str(c.get("id")) for c in chunk_rows if c.get("id")]
            if chunk_ids and ReverseLookupAssembler:
                chunk_ctx, chunk_sources, chunk_tokens = ContextManager.build_context_with_chunk_reverse_lookup(
                    project_id, chunk_ids, current_arc_id, token_limit=8000
                )
                if chunk_ctx:
                    context_parts.append(chunk_ctx)
                    total_tokens += chunk_tokens
                    sources.extend(chunk_sources)
                    sources.append("üì¶ Chunk + Reverse Lookup")
            if not chunk_ids:
                # Fallback: search bible
                intent = "search_bible"

        if intent == "search_bible" or intent == "mixed_context":
            raw_inferred = router_result.get("inferred_prefixes") or []
            valid_keys = Config.get_valid_prefix_keys()
            inferred_prefixes = [
                p for p in raw_inferred
                if p and str(p).strip().upper().replace(" ", "_") in valid_keys
            ] if valid_keys else raw_inferred
            bible_context = ""
            for entity in target_bible_entities:
                raw_list = HybridSearch.smart_search_hybrid_raw(
                    entity, project_id, top_k=2, inferred_prefixes=inferred_prefixes
                )
                if raw_list:
                    for item in raw_list:
                        try:
                            eid = item.get("id")
                            if eid is not None:
                                HybridSearch.update_lookup_stats(eid)
                        except Exception:
                            pass
                    main_id = raw_list[0].get("id") if raw_list else None
                    rel_block = ""
                    if main_id:
                        rel_text = ContextManager.get_entity_relations(main_id, project_id)
                        if rel_text:
                            rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                    part = format_bible_context_by_sections(raw_list)
                    bible_context += f"\n--- {entity.upper()} ---\n{rel_block}{part}\n"

            if not bible_context and router_result.get("rewritten_query"):
                raw_list = HybridSearch.smart_search_hybrid_raw(
                    router_result["rewritten_query"],
                    project_id,
                    top_k=5,
                    inferred_prefixes=inferred_prefixes,
                )
                if raw_list:
                    for item in raw_list:
                        try:
                            eid = item.get("id")
                            if eid is not None:
                                HybridSearch.update_lookup_stats(eid)
                        except Exception:
                            pass
                    main_id = raw_list[0].get("id") if raw_list else None
                    rel_block = ""
                    if main_id:
                        rel_text = ContextManager.get_entity_relations(main_id, project_id)
                        if rel_text:
                            rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                    part = format_bible_context_by_sections(raw_list)
                    bible_context = f"\n--- KNOWLEDGE BASE ---\n{rel_block}{part}\n"

            if bible_context:
                context_parts.append(bible_context)
                total_tokens += AIService.estimate_tokens(bible_context)
                sources.append("üìö Bible Search")

            try:
                services = init_services()
                supabase = services['supabase']
                related_chapter_nums = set()

                if target_bible_entities:
                    for entity in target_bible_entities:
                        res = supabase.table("story_bible") \
                            .select("source_chapter") \
                            .eq("story_id", project_id) \
                            .ilike("entity_name", f"%{entity}%") \
                            .execute()

                        if res.data:
                            for row in res.data:
                                if row.get('source_chapter') and row['source_chapter'] > 0:
                                    related_chapter_nums.add(row['source_chapter'])

                if related_chapter_nums:
                    chap_res = supabase.table("chapters") \
                        .select("title") \
                        .eq("story_id", project_id) \
                        .in_("chapter_number", list(related_chapter_nums)) \
                        .execute()

                    if chap_res.data:
                        auto_files = [c['title'] for c in chap_res.data if c.get('title')]

                        if auto_files:
                            extra_text, extra_sources = ContextManager.load_full_content(auto_files, project_id)

                            if extra_text:
                                context_parts.append(f"\n--- üïµÔ∏è AUTO-DETECTED CONTEXT (REVERSE LOOKUP) ---\n{extra_text}")
                                sources.extend([f"{s} (Auto)" for s in extra_sources])
                                total_tokens += AIService.estimate_tokens(extra_text)

            except Exception as e:
                print(f"Reverse lookup error: {e}")
                pass

        if intent == "mixed_context" and target_files:
            full_text, source_names = ContextManager.load_full_content(
                target_files, project_id,
                token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
            )
            if full_text:
                context_parts.append(f"\n--- RELATED FILES ---\n{full_text}")
                sources.extend(source_names)
                total_tokens += AIService.estimate_tokens(full_text)

        context_str = "\n".join(context_parts)
        if max_context_tokens is not None and total_tokens > max_context_tokens:
            context_str, total_tokens = cap_context_to_tokens(context_str, max_context_tokens)
        return context_str, sources, total_tokens


# ==========================================
# üìù AUTO-SUMMARY / CHAPTER METADATA (V5)
# ==========================================
def suggest_import_category(text: str) -> str:
    """G·ª£i √Ω prefix/category cho n·ªôi dung import (d√πng LLM nh·∫π). D√πng prefix t·ª´ DB (get_prefixes), tr·∫£ v·ªÅ [OTHER] n·∫øu kh√¥ng kh·ªõp."""
    if not text or len(text.strip()) < 20:
        return "[OTHER]"
    try:
        model = getattr(Config, "METADATA_MODEL", None) or "google/gemini-2.5-flash"
        prefixes = Config.get_prefixes()
        if not prefixes:
            return "[OTHER]"
        if "[OTHER]" not in prefixes:
            prefixes = list(prefixes) + ["[OTHER]"]
        prompt = f"""Ph√¢n lo·∫°i n·ªôi dung sau v√†o ƒê√öNG M·ªòT trong c√°c lo·∫°i (ch·ªâ tr·∫£ v·ªÅ chu·ªói lo·∫°i, kh√¥ng gi·∫£i th√≠ch):
{', '.join(prefixes)}

N·ªòI DUNG (r√∫t g·ªçn):
{text[:1500]}

Tr·∫£ v·ªÅ ƒë√∫ng m·ªôt chu·ªói, v√≠ d·ª•: [CHARACTER] ho·∫∑c [RULE]."""
        resp = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.1,
            max_tokens=50,
        )
        raw = (resp.choices[0].message.content or "").strip()
        for p in prefixes:
            if p in raw or (p.strip("[]") and p.strip("[]").lower() in raw.lower()):
                return p
        return "[OTHER]"
    except Exception as e:
        print(f"suggest_import_category error: {e}")
        return "[OTHER]"


def generate_arc_summary_from_chapters(chapter_summaries: List[Dict[str, Any]], arc_name: str = "") -> Optional[str]:
    """T·ª´ danh s√°ch t√≥m t·∫Øt ch∆∞∆°ng, AI t·∫°o t√≥m t·∫Øt ng·∫Øn cho Arc. Tr·∫£ v·ªÅ str ho·∫∑c None n·∫øu l·ªói."""
    if not chapter_summaries or not isinstance(chapter_summaries, list):
        return None
    parts = []
    for i, ch in enumerate(chapter_summaries):
        num = ch.get("chapter_number") or ch.get("num") or (i + 1)
        summ = ch.get("summary") or ch.get("description") or ""
        if summ:
            parts.append(f"Ch∆∞∆°ng {num}: {summ}")
    if not parts:
        return None
    combined = "\n".join(parts)
    try:
        model = getattr(Config, "METADATA_MODEL", None) or "google/gemini-2.5-flash"
        prompt = f"""C√°c t√≥m t·∫Øt ch∆∞∆°ng thu·ªôc Arc '{arc_name or 'Unnamed'}':

{combined}

Nhi·ªám v·ª•: Vi·∫øt 1 ƒëo·∫°n t√≥m t·∫Øt ng·∫Øn g·ªçn (2-5 c√¢u) cho to√†n b·ªô Arc, n·ªëi m·∫°ch c√°c s·ª± ki·ªán/t√¨nh ti·∫øt ch√≠nh. Ch·ªâ tr·∫£ v·ªÅ ƒëo·∫°n t√≥m t·∫Øt, kh√¥ng l·ªùi d·∫´n."""
        resp = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.3,
            max_tokens=500,
        )
        raw = (resp.choices[0].message.content or "").strip()
        return raw if raw else None
    except Exception as e:
        print(f"generate_arc_summary_from_chapters error: {e}")
        return None


def generate_chapter_metadata(content: str) -> Dict[str, str]:
    """D√πng model r·∫ª (gemini/haiku/deepseek) ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung v√† ph√¢n t√≠ch art_style. Tr·∫£ v·ªÅ {"summary": "...", "art_style": "..."}. Defensive: tr·∫£ v·ªÅ dict r·ªóng n·∫øu l·ªói."""
    if not content or not str(content).strip():
        return {"summary": "", "art_style": ""}
    try:
        model = getattr(Config, "METADATA_MODEL", None) or "google/gemini-2.5-flash"
        prompt = f"""Ph√¢n t√≠ch ƒëo·∫°n vƒÉn/ch∆∞∆°ng sau v√† tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON v·ªõi 2 key:
- "summary": T√≥m t·∫Øt n·ªôi dung (2-4 c√¢u, ti·∫øng Vi·ªát).
- "art_style": Phong c√°ch vi·∫øt (v√≠ d·ª•: k·ªÉ chuy·ªán, m√¥ t·∫£, ƒë·ªëi tho·∫°i, h√†nh ƒë·ªông; 1-2 c√¢u).

N·ªòI DUNG:
{content[:12000]}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch. V√≠ d·ª•: {{"summary": "...", "art_style": "..."}}"""
        messages = [{"role": "user", "content": prompt}]
        response = AIService.call_openrouter(
            messages=messages,
            model=model,
            temperature=0.2,
            max_tokens=500,
            response_format={"type": "json_object"},
        )
        raw = response.choices[0].message.content
        raw = AIService.clean_json_text(raw)
        data = json.loads(raw)
        return {
            "summary": str(data.get("summary", ""))[:2000],
            "art_style": str(data.get("art_style", ""))[:500],
        }
    except Exception as e:
        print(f"generate_chapter_metadata error: {e}")
        return {"summary": "", "art_style": ""}


def get_file_sample(file_content: str, sample_size: int = 80) -> str:
    """
    L·∫•y m·∫´u r·∫£i r√°c: 80 d√≤ng ƒë·∫ßu + 80 d√≤ng gi·ªØa + 80 d√≤ng cu·ªëi (n·∫øu file d√†i).
    Tr·∫£ v·ªÅ chu·ªói k·∫øt h·ª£p v·ªõi marker [ƒê·∫¶U], [GI·ªÆA], [CU·ªêI].
    """
    if not file_content or not str(file_content).strip():
        return ""
    lines = str(file_content).strip().splitlines()
    total_lines = len(lines)
    if total_lines <= sample_size * 3:
        return "\n".join(lines)
    parts = []
    parts.append(f"[ƒê·∫¶U FILE - {sample_size} d√≤ng ƒë·∫ßu]")
    parts.append("\n".join(lines[:sample_size]))
    mid_start = total_lines // 2 - sample_size // 2
    parts.append(f"\n\n[GI·ªÆA FILE - {sample_size} d√≤ng gi·ªØa (t·ª´ d√≤ng {mid_start})]")
    parts.append("\n".join(lines[mid_start:mid_start + sample_size]))
    parts.append(f"\n\n[CU·ªêI FILE - {sample_size} d√≤ng cu·ªëi]")
    parts.append("\n".join(lines[-sample_size:]))
    return "\n".join(parts)


def analyze_split_strategy(
    file_content: str,
    file_type: str = "story",
    context_hint: str = "",
) -> Dict[str, Any]:
    """
    AI Analyzer (Nh·∫π): Ph√¢n t√≠ch m·∫´u r·∫£i r√°c (80 ƒë·∫ßu + 80 gi·ªØa + 80 cu·ªëi) ƒë·ªÉ t√¨m quy lu·∫≠t ph√¢n c√°ch.
    Tr·∫£ v·ªÅ {"split_type": "by_keyword"|"by_length"|"by_sheet", "split_value": str (regex/keyword)}.
    """
    if not file_content or not str(file_content).strip():
        return {"split_type": "by_length", "split_value": "2000"}
    sample = get_file_sample(file_content, sample_size=80)
    try:
        model = getattr(Config, "METADATA_MODEL", None) or "google/gemini-2.5-flash"
        type_hints = {
            "story": "Truy·ªán - t√¨m quy lu·∫≠t ph√¢n c√°ch ch∆∞∆°ng (VD: 'Ch∆∞∆°ng' vi·∫øt hoa, d·∫•u '***', xu·ªëng d√≤ng 2 l·∫ßn).",
            "character_data": "D·ªØ li·ªáu nh√¢n v·∫≠t - t√¨m quy lu·∫≠t ph√¢n c√°ch entity (VD: '##', '---', t√™n ri√™ng ·ªü ƒë·∫ßu d√≤ng).",
            "excel_export": "Excel/CSV - x√°c ƒë·ªãnh c·∫Øt theo 'Sheet' marker hay 'Row count' (s·ªë d√≤ng c·ªë ƒë·ªãnh).",
        }
        hint_text = type_hints.get(file_type.strip().lower(), type_hints["story"])
        if context_hint:
            hint_text += f"\nG·ª£i √Ω ng∆∞·ªùi d√πng: {context_hint}"
        prompt = f"""Ph√¢n t√≠ch m·∫´u file (80 d√≤ng ƒë·∫ßu + 80 d√≤ng gi·ªØa + 80 d√≤ng cu·ªëi) v√† T√åM QUY LU·∫¨T PH√ÇN C√ÅCH.

Lo·∫°i file: {hint_text}

M·∫™U FILE (240 d√≤ng t·ªïng h·ª£p):
---
{sample}
---

NHI·ªÜM V·ª§: T√¨m quy lu·∫≠t ph√¢n c√°ch ch∆∞∆°ng/th·ª±c th·ªÉ/sheet trong file n√†y.
- V√≠ d·ª•: "Ch∆∞∆°ng" vi·∫øt hoa ·ªü ƒë·∫ßu d√≤ng, d·∫•u "***", xu·ªëng d√≤ng 2 l·∫ßn, "[Sheet: X]", v.v.

Y√äU C·∫¶U: Tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON v·ªõi:
- "split_type": m·ªôt trong ["by_keyword", "by_length", "by_sheet"]
  * "by_keyword": T√¨m th·∫•y t·ª´ kh√≥a/pattern l·∫∑p l·∫°i ‚Üí tr·∫£ v·ªÅ regex pattern ho·∫∑c keyword ƒë∆°n gi·∫£n
  * "by_length": Kh√¥ng t√¨m th·∫•y pattern r√µ r√†ng ‚Üí c·∫Øt theo s·ªë k√Ω t·ª± c·ªë ƒë·ªãnh
  * "by_sheet": File Excel ‚Üí c·∫Øt theo Sheet marker
- "split_value": 
  * N·∫øu by_keyword: Regex pattern (VD: "^Ch∆∞∆°ng\\s+\\d+", "\\*{3,}", "^##\\s+") ho·∫∑c keyword ƒë∆°n gi·∫£n (VD: "Ch∆∞∆°ng", "---")
  * N·∫øu by_length: s·ªë k√Ω t·ª± (VD: "2000")
  * N·∫øu by_sheet: "Sheet" ho·∫∑c "Row count"

QUAN TR·ªåNG: Ch·ªâ tr·∫£ v·ªÅ Regex pattern ho·∫∑c Keyword ƒë·ªÉ Python d√πng `re` module c·∫Øt file. KH√îNG c·∫Øt th·ª±c t·∫ø.

V√≠ d·ª•: {{"split_type": "by_keyword", "split_value": "^Ch∆∞∆°ng\\s+\\d+"}}
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch."""

        response = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.2,
            max_tokens=500,
            response_format={"type": "json_object"},
        )
        raw = (response.choices[0].message.content or "").strip()
        raw = AIService.clean_json_text(raw)
        data = json.loads(raw)
        split_type = data.get("split_type", "by_length")
        split_value = str(data.get("split_value", "2000")).strip()
        if split_type not in ["by_keyword", "by_length", "by_sheet"]:
            split_type = "by_length"
        return {"split_type": split_type, "split_value": split_value}
    except Exception as e:
        print(f"analyze_split_strategy error: {e}")
        return {"split_type": "by_length", "split_value": "2000"}


def _build_smart_regex_pattern(keyword: str) -> str:
    """
    X√¢y d·ª±ng regex pattern h·ªó tr·ª£ c√≥ d·∫•u/kh√¥ng d·∫•u v√† kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng.
    VD: "Ch∆∞∆°ng" -> r"(?i)(CH∆Ø∆†NG|CHUONG|CHAPTER)\s+\d+[:\s-]*"
    """
    import re
    keyword_upper = keyword.upper().strip()
    if keyword_upper in ["CH∆Ø∆†NG", "CHUONG", "CHAPTER"]:
        return r"(?i)(CH∆Ø∆†NG|CHUONG|CHAPTER)\s+\d+[:\s-]*"
    elif keyword_upper in ["PH·∫¶N", "PHAN", "PART"]:
        return r"(?i)(PH·∫¶N|PHAN|PART)\s+\d+[:\s-]*"
    elif keyword_upper in ["---", "***", "==="]:
        return rf"(?i)\s*{re.escape(keyword)}\s*"
    else:
        return rf"(?i)^\s*{re.escape(keyword)}\s*"


def execute_split_logic(
    file_content: str,
    split_type: str,
    split_value: str,
    debug: bool = False,
) -> List[Dict[str, Any]]:
    """
    Python Worker (M·∫°nh): C·∫Øt file b·∫±ng code Python thu·∫ßn, kh√¥ng g·ªçi AI.
    Tr·∫£ v·ªÅ list of {"title": str, "content": str, "order": int}.
    debug=True: In ra debug log (d√πng trong Streamlit v·ªõi st.write).
    """
    if not file_content or not str(file_content).strip():
        return []
    content = str(file_content).strip()
    out = []
    try:
        if split_type == "by_keyword":
            import re
            pattern_str = split_value.strip()
            if not pattern_str:
                pattern_str = "---"
            
            is_regex = any(c in pattern_str for c in ["^", "$", "\\d", "\\s", "\\w", "\\+", "\\*", "\\?", "\\[", "\\(", "\\{", "("])
            
            if not is_regex:
                pattern_str = _build_smart_regex_pattern(pattern_str)
                is_regex = True
            
            try:
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
            except Exception as e:
                if debug:
                    print(f"Regex compile error: {e}, fallback to simple pattern")
                pattern_str = rf"^\s*{re.escape(split_value.strip())}\s*"
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
            
            matches = list(pattern.finditer(content))
            if debug:
                import streamlit as st
                st.write(f"üîç **Debug Log:** T√¨m th·∫•y **{len(matches)}** v·ªã tr√≠ ph√¢n c√°ch:")
                for i, m in enumerate(matches[:10]):
                    line_num = content[:m.start()].count('\n') + 1
                    preview = content[max(0, m.start()-30):m.end()+30].replace('\n', ' ')
                    st.code(f"{i+1}. D√≤ng {line_num}: ...{preview}...", language=None)
                if len(matches) > 10:
                    st.caption(f"... v√† {len(matches) - 10} v·ªã tr√≠ kh√°c")
            
            if len(matches) == 0:
                if debug:
                    import streamlit as st
                    st.error("‚ùå **Kh√¥ng t√¨m th·∫•y d·∫•u hi·ªáu ph√¢n chia ch∆∞∆°ng.** Vui l√≤ng ki·ªÉm tra l·∫°i ƒë·ªãnh d·∫°ng ho·∫∑c th·ª≠ keyword/pattern kh√°c.")
                return []
            
            # Ph·∫ßn tr∆∞·ªõc t·ª´ kh√≥a ƒë·∫ßu (n·∫øu c√≥)
            if matches[0].start() > 0:
                part_content = content[0:matches[0].start()].strip()
                if part_content:
                    title = "Ph·∫ßn m·ªü ƒë·∫ßu" if not out else "Ph·∫ßn 0"
                    out.append({"title": title, "content": part_content, "order": 1})
            
            # N·ªôi dung N·∫∞M GI·ªÆA hai t·ª´ kh√≥a: t·ª´ sau keyword[i] ƒë·∫øn tr∆∞·ªõc keyword[i+1]
            for i, match in enumerate(matches):
                start = match.end()  # B·∫Øt ƒë·∫ßu SAU t·ª´ kh√≥a hi·ªán t·∫°i
                end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
                part_content = content[start:end].strip()
                if not part_content:
                    continue
                title = match.group(0).strip()[:50] if match.group(0) else f"Ph·∫ßn {len(out)+1}"
                if not title or len(title.strip()) < 2:
                    first_line = part_content.splitlines()[0] if part_content.splitlines() else ""
                    title = first_line[:50] if first_line else f"Ph·∫ßn {len(out)+1}"
                out.append({"title": title, "content": part_content, "order": len(out) + 1})
        elif split_type == "by_length":
            chunk_size = int(split_value) if split_value.isdigit() else 2000
            chunk_size = max(500, min(chunk_size, 50000))
            lines = content.splitlines()
            current_chunk = []
            current_len = 0
            chunk_num = 1
            for line in lines:
                line_len = len(line) + 1
                if current_len + line_len > chunk_size and current_chunk:
                    chunk_text = "\n".join(current_chunk).strip()
                    if chunk_text:
                        out.append({"title": f"Ph·∫ßn {chunk_num}", "content": chunk_text, "order": chunk_num})
                        chunk_num += 1
                    current_chunk = [line]
                    current_len = line_len
                else:
                    current_chunk.append(line)
                    current_len += line_len
            if current_chunk:
                chunk_text = "\n".join(current_chunk).strip()
                if chunk_text:
                    out.append({"title": f"Ph·∫ßn {chunk_num}", "content": chunk_text, "order": chunk_num})
        elif split_type == "by_sheet":
            import re
            if split_value.lower() == "row count" or split_value.isdigit():
                row_count = int(split_value) if split_value.isdigit() else 100
                lines = content.splitlines()
                for i in range(0, len(lines), row_count):
                    chunk_lines = lines[i:i + row_count]
                    if chunk_lines:
                        out.append({"title": f"Sheet {i // row_count + 1}", "content": "\n".join(chunk_lines), "order": i // row_count + 1})
            elif "[Sheet:" in content or "[Sheet " in content:
                pattern = re.compile(r"\[Sheet[:\s]+([^\]]+)\]", re.IGNORECASE)
                parts = pattern.split(content)
                current_sheet = "Sheet 1"
                current_content = []
                idx = 0
                for i, part in enumerate(parts):
                    if i % 2 == 0:
                        if part.strip():
                            current_content.append(part.strip())
                    else:
                        if current_content:
                            out.append({"title": current_sheet, "content": "\n".join(current_content), "order": idx + 1})
                            idx += 1
                        current_sheet = part.strip() or f"Sheet {idx + 2}"
                        current_content = []
                if current_content:
                    out.append({"title": current_sheet, "content": "\n".join(current_content), "order": idx + 1})
            else:
                out.append({"title": "Ph·∫ßn 1", "content": content, "order": 1})
        else:
            out.append({"title": "Ph·∫ßn 1", "content": content, "order": 1})
        return out
    except Exception as e:
        print(f"execute_split_logic error: {e}")
        return [{"title": "Ph·∫ßn 1", "content": content, "order": 1}]


# ==========================================
# üß¨ RULE MINING SYSTEM
# ==========================================
class RuleMiningSystem:
    """H·ªá th·ªëng khai th√°c v√† qu·∫£n l√Ω lu·∫≠t t·ª´ chat"""

    @staticmethod
    def extract_rule_raw(user_prompt: str, ai_response: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t lu·∫≠t th√¥ t·ª´ h·ªôi tho·∫°i"""
        prompt = f"""
        B·∫°n l√† "Trinh S√°t Lu·∫≠t" (Rule Scout). Nhi·ªám v·ª•: Ph√°t hi·ªán s·ªü th√≠ch/y√™u c·∫ßu c·ªßa User.

        H·ªòI THO·∫†I:
        - User: "{user_prompt}"
        - AI: (Ph·∫£n h·ªìi tr∆∞·ªõc ƒë√≥...)

        M·ª§C TI√äU:
        Ph√°t hi·ªán xem User c√≥ ƒëang ng·∫ßm ch·ªâ ƒë·ªãnh C√ÅCH L√ÄM VI·ªÜC, C√ÅCH VI·∫æT, ho·∫∑c ƒê·ªäNH D·∫†NG kh√¥ng.

        TI√äU CH√ç (ƒê·ªô nh·∫°y cao):
        1. Y√™u c·∫ßu ƒë·ªãnh d·∫°ng: "ch·ªâ json", "d√πng markdown", "ƒë·ª´ng vi·∫øt code", "vi·∫øt ng·∫Øn th√¥i".
        2. ƒêi·ªÅu ch·ªânh vƒÉn phong: "nghi√™m t√∫c h∆°n", "b·ªõt n√≥i nh·∫£m", "d√πng ti·∫øng Vi·ªát".
        3. S·ª≠a l·ªói: "sai r·ªìi", "kh√¥ng ph·∫£i th·∫ø", "l√†m th·∫ø n√†y m·ªõi ƒë√∫ng".

        H∆Ø·ªöNG D·∫™N:
        - N·∫øu User n√≥i: "Vi·∫øt c√°i n√†y b·∫±ng Python nh√©" -> T·∫°o lu·∫≠t: "Lu√¥n ∆∞u ti√™n d√πng Python".
        - Th√† b·∫Øt nh·∫ßm c√≤n h∆°n b·ªè s√≥t.

        OUTPUT:
        - N·∫øu ph√°t hi·ªán lu·∫≠t: Tr·∫£ v·ªÅ 1 c√¢u m·ªánh l·ªánh ng·∫Øn g·ªçn k√®m ng·ªØ c·∫£nh (Ti·∫øng Vi·ªát). V√≠ d·ª•: "Lu√¥n tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON khi ƒë∆∞·ª£c y√™u c·∫ßu...", "Kh√¥ng gi·∫£i th√≠ch d√†i d√≤ng khi user ƒëang kh√≥ ch·ªãu...".
        - N·∫øu ch·ªâ l√† ch√†o h·ªèi/c·∫£m ∆°n: Tr·∫£ v·ªÅ "NO_RULE".

        Ch·ªâ tr·∫£ v·ªÅ Text.
        """

        messages = [
            {"role": "system", "content": "You are Rule Extractor. Return text only."},
            {"role": "user", "content": prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.3,
                max_tokens=300
            )

            text = response.choices[0].message.content.strip()

            if "NO_RULE" in text or len(text) < 5:
                return None
            return text
        except Exception as e:
            print(f"Rule extraction error: {e}")
            return None

    @staticmethod
    def analyze_rule_conflict(new_rule_content: str, project_id: str) -> Dict:
        """Check rule conflict with DB - Safe Version"""
        similar_rules_str = HybridSearch.smart_search_hybrid(new_rule_content, project_id, top_k=3)

        if not similar_rules_str:
            return {
                "status": "NEW",
                "reason": "No conflicts found",
                "existing_rule_summary": "None",
                "merged_content": None,
                "suggested_content": new_rule_content
            }

        judge_prompt = f"""
        Lu·∫≠t M·ªõi: "{new_rule_content}"
        Lu·∫≠t C≈© trong DB: "{similar_rules_str}"

        Nhi·ªám v·ª•: So s√°nh m·ªëi quan h·ªá.

        - CONFLICT (Xung ƒë·ªôt): M√¢u thu·∫´n tr·ª±c ti·∫øp (Vd: C≈© b·∫£o A, M·ªõi b·∫£o kh√¥ng A).
        - MERGE (G·ªôp): C√πng ch·ªß ƒë·ªÅ nh∆∞ng lu·∫≠t M·ªõi chi ti·∫øt h∆°n ho·∫∑c b·ªï sung cho lu·∫≠t C≈©.
        - NEW (M·ªõi): Ch·ªß ƒë·ªÅ kh√°c h·∫≥n.

        OUTPUT JSON ONLY:
        {{
            "status": "CONFLICT" | "MERGE" | "NEW",
            "existing_rule_summary": "T√≥m t·∫Øt lu·∫≠t c≈© (Ti·∫øng Vi·ªát)",
            "reason": "L√Ω do (Ti·∫øng Vi·ªát)",
            "merged_content": "N·ªôi dung lu·∫≠t ƒë√£ g·ªôp ho√†n ch·ªânh (n·∫øu MERGE). N·∫øu kh√°c th√¨ ƒë·ªÉ null."
        }}
        """

        messages = [
            {"role": "system", "content": "You are Rule Judge. Return only JSON."},
            {"role": "user", "content": judge_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.2,
                max_tokens=4000,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            content = AIService.clean_json_text(content)

            result = json.loads(content)

            return {
                "status": result.get("status", "NEW"),
                "reason": result.get("reason", "No reason provided by AI"),
                "existing_rule_summary": result.get("existing_rule_summary", "N/A"),
                "merged_content": result.get("merged_content", None),
                "suggested_content": new_rule_content
            }

        except Exception as e:
            print(f"Rule analysis error: {e}")
            return {
                "status": "NEW",
                "reason": f"AI Judge Error: {str(e)}",
                "existing_rule_summary": "Error analyzing",
                "merged_content": None,
                "suggested_content": new_rule_content
            }

    @staticmethod
    def crystallize_session(chat_history: List[Dict], persona_role: str) -> str:
        """T√≥m t·∫Øt v√† l·ªçc th√¥ng tin gi√° tr·ªã t·ª´ chat history"""
        chat_text = "\n".join([f"{m['role']}: {m['content']}" for m in chat_history])

        crystallize_prompt = f"""
        B·∫°n l√† Th∆∞ K√Ω Cu·ªôc H·ªçp ({persona_role}).
        
        Nhi·ªám v·ª•: ƒê·ªçc ƒëo·∫°n chat d∆∞·ªõi ƒë√¢y v√† L·ªåC B·ªé NH·ªÆNG TH·ª® V√î NGHƒ®A.
        Ch·ªâ gi·ªØ l·∫°i v√† T√ìM T·∫ÆT nh·ªØng th√¥ng tin gi√° tr·ªã (S·ª± ki·ªán, √ù t∆∞·ªüng, Quy·∫øt ƒë·ªãnh, Lore m·ªõi).

        CHAT LOG: {chat_text}

        OUTPUT: Tr·∫£ v·ªÅ b·∫£n t√≥m t·∫Øt s√∫c t√≠ch (50-100 t·ª´) b·∫±ng Ti·∫øng Vi·ªát. 
        N·∫øu to√†n l√† ch√†o h·ªèi v√¥ nghƒ©a, tr·∫£ v·ªÅ "NO_INFO".
        """

        messages = [
            {"role": "system", "content": "You are Conversation Summarizer. Return text only."},
            {"role": "user", "content": crystallize_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.3,
                max_tokens=8000
            )

            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Crystallize error: {e}")
            return f"AI Error: {e}"
